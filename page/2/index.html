<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>jerrychen的博客</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="jerrychen的博客"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="jerrychen的博客"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="jerrychen的博客"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="jerrychen的博客"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="jerrychen"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"jerrychen的博客","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"jerrychen"},"publisher":{"@type":"Organization","name":"jerrychen的博客","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.15.2/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?e7c8dc4aeff2a6cff0b51140680831cd";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="jerrychen的博客" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-03-09T16:00:00.000Z" title="3/10/2020, 12:00:00 AM">2020-03-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-12-20T07:40:57.905Z" title="12/20/2021, 3:40:57 PM">2021-12-20</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">2 minutes read (About 318 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/03/10/nlp/TextFromKG/">《Text Generation from Knowledge Graphs with Graph Transformers》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.02342.pdf">论文链接</a></p>
<ul>
<li>解决问题：给定一篇论文的title +通过知识抽取工具从论文abstract里抽取出的知识图谱，用生成式模型生成文章的abstract</li>
<li>整体框架：用BiLSTM encode title, 用Graph Transformer encode 知识图谱。 decode过程中同时可以attention到title和图谱的encode特征，同时加上copy机制</li>
<li>细节：graph transformer用节点相邻的节点作为该节点的context，其他和text transformer类似</li>
<li>细节：原始图谱的边是有标签且无向的，在做graph transformer之前，将原始图的边改造成两个节点，这样得到的图的边是有向、无标签的。 同时加上一个global的节点，和所有节点都连接，让整个图联通</li>
<li>数据集：自建数据集，包含40k论文</li>
<li>评价指标：人工打分+BLUE+METEOR</div><a class="article-more button is-small is-size-7" href="/2020/03/10/nlp/TextFromKG/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-01-16T16:00:00.000Z" title="1/17/2020, 12:00:00 AM">2020-01-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-12-20T07:44:00.121Z" title="12/20/2021, 3:44:00 PM">2021-12-20</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">2 minutes read (About 358 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/01/17/nlp/survey-OPENIE/">《A Survey on Open Information Extraction》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.05599.pdf">论文链接</a></p>
<ul>
<li>information extraction从文本中抽取出SPO三元组,传统的information extraction都是抽取事先给定的关系</li>
<li>Open information extraction(Open IE)的关系无需实现给定，能够自动从大量的文本中发掘出关系(关系可能是原文中的span，也可能不是)</li>
<li>OPEN IE的三个挑战：<ul>
<li>Automation：需要手动标注的数据必须限制在较小的数量级</li>
<li>Corpus Heterogeneity:能在不同分布的数据集上work，不能依赖领域相关的信息，比如NER。只能用POS这些浅层tag</li>
<li>Efficiency：需要在大量数据上运行，需要预测性能高，只能依赖POStag这些浅层信息</li>
</ul>
</li>
<li>OPEN IE的方法：<ul>
<li>Learning-based System: TEXTRUNNER/WOE/OLLIE</li>
<li>Rule-based System:利用语言学、统计学特征+规则 PredPatt</li>
<li>Clause-based System: Stanford OpenIE</li>
<li>Systems Capturing Inter-Proposition Relationships: 同时抽取三元组以及原文中三元组成立的前提</div><a class="article-more button is-small is-size-7" href="/2020/01/17/nlp/survey-OPENIE/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-11-18T16:00:00.000Z" title="11/19/2019, 12:00:00 AM">2019-11-19</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-12-20T07:46:34.929Z" title="12/20/2021, 3:46:34 PM">2021-12-20</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">2 minutes read (About 272 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/11/19/nlp/KG-BERT/">《KG-BERT: BERT for Knowledge Graph Completion》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.03193.pdf">论文链接</a></p>
<ul>
<li>目标：做KG-completion</li>
<li>思路：基于预训练的BERT，做SPO三元组的embedding， 不依赖原句</li>
<li>训练<ul>
<li>将SPO三元组拼接成[CLS]S[SEP]P[SEP]O[SEP]的形式， 其中S和O用同样的segment embedding</li>
<li>S和O是entity的name或者description</li>
<li>用二分类判断三元组是否正确， 或者用多分类，给定S\O 判断relation的类型</li>
</ul>
</li>
<li>预测<ul>
<li>给定SPO，判断是否正确</li>
<li>给定SO，判断relation在schema中的哪一个</li>
<li>给定 SP，判断O是哪个（将所有可能的O列举，拼接成三元组之后预测），按照得分排序取第一个</div><a class="article-more button is-small is-size-7" href="/2019/11/19/nlp/KG-BERT/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-11-06T16:00:00.000Z" title="11/7/2019, 12:00:00 AM">2019-11-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-12-20T07:31:48.339Z" title="12/20/2021, 3:31:48 PM">2021-12-20</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">4 minutes read (About 545 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/11/07/nlp/GPT/">《Improving Language Understanding by Generative Pre-Training》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">论文链接</a></p>
<ul>
<li>目标：用与训练的LM模型提升NLU任务的效果</li>
<li>基于大量未标注语料训练的两个问题<ul>
<li>如何设置合理的训练目标？LM/NMT/discourse coherence? 这也是GPT和bert的区别之一</li>
<li>如何将预训练的模型得到的表征应用到下游任务中去？</li>
</ul>
</li>
<li>模型结构<ul>
<li>一个没有encoder-attention的transformer decoder</li>
<li>给定一个窗口的输入(最后n个token)，预测下一个单词</li>
<li>模型返回的是最后一个token对应的embedding，而不是真个窗口sequence的embedding</div><a class="article-more button is-small is-size-7" href="/2019/11/07/nlp/GPT/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-11-06T16:00:00.000Z" title="11/7/2019, 12:00:00 AM">2019-11-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-12-20T07:37:16.906Z" title="12/20/2021, 3:37:16 PM">2021-12-20</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">4 minutes read (About 608 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/11/07/nlp/survey-NER/">《A Survey on Deep Learning forNamed Entity Recognition》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.09449.pdf">论文链接</a></p>
<ul>
<li>NER是信息抽取、问答系统、机器翻译的一项基础工作，DNN的应用让NER任务有了长足的进步</li>
<li>NER分为两类coarse-grained NER:比较粗粒度的划分entity，比如通用NER。 fine-grained NER：更加细分的实体类型，通常是和具体的业务相关的实体，一个mention可以属于多个实体类别</li>
<li>数据集：见原文table1。 比较常用的有：<ul>
<li>OntoNotes：18 coarse entity type consisting of 89 subtype</li>
<li>CoNLL03 4 entity types</li>
</ul>
</li>
<li>工具：见原文table2 StanfordCoreNLP/NLTK/spaCy</li>
<li>评价指标：<ul>
<li>exact-match evaluation： 用全匹配方法计算F1。会有些偏严，指标偏低</li>
<li>relaxed-match evaluation: 宽松匹配方案，不太好控制</div><a class="article-more button is-small is-size-7" href="/2019/11/07/nlp/survey-NER/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-10-30T16:00:00.000Z" title="10/31/2019, 12:00:00 AM">2019-10-31</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-01-05T09:42:42.389Z" title="1/5/2022, 5:42:42 PM">2022-01-05</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">4 minutes read (About 609 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/10/31/nlp/GPT-Chinese-poetry/">《GPT-based Generation for Classical Chinese Poetry》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.00151.pdf">论文链接</a></p>
<ul>
<li>目标：根据输入的格式（对联、绝句、律诗、词牌名） + 主体（诗名、词名、藏头诗的头），生成相应格式的对联、诗、词</li>
<li>诗歌生成的难点：<ul>
<li>生成文本需要满足相应的诗歌类型的格式（长度、对偶、押韵、平仄等）</li>
<li>生成的文本需要主题一致，如果给定主题的话，需要和给定主题一样</li>
</ul>
</li>
<li>之前的做法：<ul>
<li>用基于constraint 或者基于template的方式满足格式</li>
<li>用插入关键词的方式满足主题一致</li>
<li>需要引入比较多的人工规则和特征</div><a class="article-more button is-small is-size-7" href="/2019/10/31/nlp/GPT-Chinese-poetry/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-10-29T16:00:00.000Z" title="10/30/2019, 12:00:00 AM">2019-10-30</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-01-05T09:42:41.377Z" title="1/5/2022, 5:42:41 PM">2022-01-05</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">2 minutes read (About 353 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/10/30/nlp/DKN-news-recommend/">《DKN: Deep Knowledge-Aware Network for News Recommendation》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1801.08284">论文链接</a></p>
<ul>
<li>目标：将知识图谱应用到新闻推荐中</li>
<li>input：用户的历史点击的新闻title+候选新闻title+通用知识图谱</li>
<li>output：ranking of candidate news</li>
<li>总体架构：<ul>
<li><img src="/images/DKN-image1.png" alt="architecture"></li>
<li>Knowledge distillation<ul>
<li>用实体链接技术，将文本中出现的实体，链接到KG中的entity</li>
<li>knowledge graph Embedding：translation-based knowledge graph embedding methods （Knowledge Graph Embedding via Dynamic Mapping Matrix）</li>
</ul>
</li>
<li>KCNN（knowledge-aware CNN）<ul>
<li>用linear mapping方法，将wordEmbedding, entityEmbedding, entityContextEmbedding 映射到相同的维度，作为输入句子的三个channel</li>
<li>用TextCNN将输入句子encode成一个vec</li>
</ul>
</li>
<li>Attention-based User Interest Extraction<ul>
<li>user点击过的所有text的embedding列表作为user的特征</li>
<li>用候选text的embedding对user点击过的text的embedding列表做attention得到最后的特征向量</li>
<li>最后接一个sigmoid，做点击率预估</div><a class="article-more button is-small is-size-7" href="/2019/10/30/nlp/DKN-news-recommend/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-09-15T16:00:00.000Z" title="9/16/2019, 12:00:00 AM">2019-09-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-12-16T08:12:51.179Z" title="12/16/2021, 4:12:51 PM">2021-12-16</time></span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%A4%E7%9F%A5/">认知</a></span><span class="level-item">22 minutes read (About 3369 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/09/16/acknowledge/%E8%87%AA%E6%8E%A7%E5%8A%9B/">《自控力》书摘</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/10786473/">豆瓣链接</a></p>
<ul>
<li><p>自控力挑战</p>
<ul>
<li>我不要：戒掉一个坏习惯</li>
<li>我要做：养成规律的作息</li>
<li>我想要：成为自由职业者</li>
</ul>
</li>
<li><p>承认自己会失控。了解什么情况下、什么原因会失控才是关键</p>
</li>
<li><p>自控力的神经学原理：前额皮质</p>
<ul>
<li>原始本能：人类进化初期积累的本能</li>
<li>自控能力：前额皮质控制的能力：我不要、我要、我想要</li>
<li>两者对抗，有时也合作，不一定要遏制原始本能，更好的利用原始本能有时事半功倍</li>
<li>有意识的做决定才能用到自控力，当你想着别的事情的时候，你的决定就是本能的、最简单的那个选择</div><a class="article-more button is-small is-size-7" href="/2019/09/16/acknowledge/%E8%87%AA%E6%8E%A7%E5%8A%9B/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-08-01T16:00:00.000Z" title="8/2/2019, 12:00:00 AM">2019-08-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-01-05T09:42:42.389Z" title="1/5/2022, 5:42:42 PM">2022-01-05</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">3 minutes read (About 474 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/08/02/nlp/MTB/">《Matching the Blanks: Distributional Similarity for Relation Learning》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1906.03158.pdf">论文链接</a></p>
<ul>
<li><p>目标：基于大量未标注语料，训练一个relation表征的模型</p>
</li>
<li><p>input：relation statement(x, s1, s2)</p>
</li>
<li><p>output：relation representation: 一个稠密向量，使得两个关系越接近，两个关系的表征向量点积值越大</p>
</li>
<li><p>bert-based architecture:<img src="/images/mtb-image1.png" alt="architecture"></p>
</li>
<li><p>预训练</p>
<ul>
<li>There is high degree for redundancy in web text, relation between tow entity is likely to be stated multiple times</li>
<li>两个不同的句子中，如果包含相同的实体对，这个实体对在两句话中大概率表示相同的relation</li>
<li>两个不同的句子中，如果包含不同实体对，这两个实体对大概率表示不同的relation</li>
<li>例子:<img src="/images/mtb-image2.png" alt="example"></li>
<li>结构:<img src="/images/mtb-image3.png" alt="architecture1"></div><a class="article-more button is-small is-size-7" href="/2019/08/02/nlp/MTB/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-05-19T16:00:00.000Z" title="5/20/2019, 12:00:00 AM">2019-05-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-12-20T07:42:51.264Z" title="12/20/2021, 3:42:51 PM">2021-12-20</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">5 minutes read (About 730 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/05/20/nlp/Transformer/">《attention is all you need》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">论文链接</a></p>
<ul>
<li><p>博客：<a target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/%E3%80%82">https://jalammar.github.io/illustrated-transformer/。</a> 原理和结构图在论文以及上面的博客讲的都很清楚，下面提一些我自己阅读论文和博客时遇到的一些疑问，以及后来自己理解觉得对的答案。有些依然没有找到答案。。。</p>
</li>
<li><p>self-attention</p>
<ul>
<li>做完key和query向量的点积之后，要除以向量维度的平方根，这样可以保持梯度比较稳定</li>
<li>为什么一定要有一个value向量，不能直接用原始向量替代么？<ul>
<li>value向量可以表示该token可以被分享到其他token的特征，和该token的embedding不一定一样。而且如果直接用原始embedding作为value的话，self-attention只是等价于之前embedding的重新打散、组合</li>
</ul>
</li>
<li>在做完multi-head之后，把多个head的embedding concat之后还要再接一个Dense层，转化成更低维度传给FFN</div><a class="article-more button is-small is-size-7" href="/2019/05/20/nlp/Transformer/#more">Read more</a></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/">Previous</a></div><div class="pagination-next"><a href="/page/3/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><a class="pagination-link is-current" href="/page/2/">2</a></li><li><a class="pagination-link" href="/page/3/">3</a></li><li><a class="pagination-link" href="/page/4/">4</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Jerrychen"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Jerrychen</p><p class="is-size-6 is-block">Developer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hanzhou/Zhejiang</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">32</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">9</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">37</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/jerrychen1990" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/jerrychen1990"><i class="fab fa-github"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/jerrychen1990" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/nlp/"><span class="level-start"><span class="level-item">nlp</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%8E%86%E5%8F%B2/"><span class="level-start"><span class="level-item">历史</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%95%86%E4%B8%9A/"><span class="level-start"><span class="level-item">商业</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"><span class="level-start"><span class="level-item">大数据</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%89%A9%E7%90%86/"><span class="level-start"><span class="level-item">物理</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%94%9F%E7%89%A9/"><span class="level-start"><span class="level-item">生物</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"><span class="level-start"><span class="level-item">计算机</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%A4%E7%9F%A5/"><span class="level-start"><span class="level-item">认知</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E9%87%91%E8%9E%8D/"><span class="level-start"><span class="level-item">金融</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-11-23T16:00:00.000Z">2021-11-24</time></p><p class="title"><a href="/2021/11/24/physics/%E4%B8%8A%E5%B8%9D%E6%8E%B7%E9%AA%B0%E5%AD%90%E4%B9%88/">《上帝掷骰子么》书摘</a></p><p class="categories"><a href="/categories/%E7%89%A9%E7%90%86/">物理</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-11-19T16:00:00.000Z">2021-11-20</time></p><p class="title"><a href="/2021/11/20/computer/LSH/">局部敏感哈希(LSH)与文本去重</a></p><p class="categories"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/">计算机</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-09-28T16:00:00.000Z">2021-09-29</time></p><p class="title"><a href="/2021/09/29/nlp/VOLT/">《Vocabulary Learning via Optimal Transport for Neural Machine Translation》</a></p><p class="categories"><a href="/categories/nlp/">nlp</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-14T16:00:00.000Z">2021-04-15</time></p><p class="title"><a href="/2021/04/15/nlp/beyond_the_accuracy/">《Beyond Accuracy: Behavioral Testing of NLP Models with CheckList》</a></p><p class="categories"><a href="/categories/nlp/">nlp</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-19T16:00:00.000Z">2021-01-20</time></p><p class="title"><a href="/2021/01/20/acknowledge/%E9%87%8D%E6%9D%A52/">《重来2》书摘</a></p><p class="categories"><a href="/categories/%E8%AE%A4%E7%9F%A5/">认知</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">November 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">September 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/04/"><span class="level-start"><span class="level-item">April 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">January 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/07/"><span class="level-start"><span class="level-item">July 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/04/"><span class="level-start"><span class="level-item">April 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/03/"><span class="level-start"><span class="level-item">March 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/01/"><span class="level-start"><span class="level-item">January 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">November 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">October 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">September 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">August 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">May 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/02/"><span class="level-start"><span class="level-item">February 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/01/"><span class="level-start"><span class="level-item">January 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/10/"><span class="level-start"><span class="level-item">October 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/05/"><span class="level-start"><span class="level-item">May 2017</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/02/"><span class="level-start"><span class="level-item">February 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/01/"><span class="level-start"><span class="level-item">January 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2016/04/"><span class="level-start"><span class="level-item">April 2016</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2016/03/"><span class="level-start"><span class="level-item">March 2016</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2015/09/"><span class="level-start"><span class="level-item">September 2015</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/BERT/"><span class="tag">BERT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Best-Paper/"><span class="tag">Best Paper</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FIRE/"><span class="tag">FIRE</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GPT/"><span class="tag">GPT</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/IE/"><span class="tag">IE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KG/"><span class="tag">KG</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NER/"><span class="tag">NER</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLG/"><span class="tag">NLG</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLU/"><span class="tag">NLU</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NMT/"><span class="tag">NMT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenIE/"><span class="tag">OpenIE</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Recommendation/"><span class="tag">Recommendation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/best-paper/"><span class="tag">best paper</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/kylin/"><span class="tag">kylin</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/self-supervised-Learning/"><span class="tag">self-supervised Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/survey/"><span class="tag">survey</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B9%A6%E6%91%98/"><span class="tag">书摘</span><span class="tag">15</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%81%A5%E8%BA%AB/"><span class="tag">健身</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%87%AF%E5%88%A9%C2%B7%E9%BA%A6%E6%A0%BC%E5%B0%BC%E6%A0%BC%E5%B0%94/"><span class="tag">凯利·麦格尼格尔</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%98%E6%9C%AA%E9%B9%8F/"><span class="tag">刘未鹏</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%90%B4%E6%99%93%E6%B3%A2/"><span class="tag">吴晓波</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BD%BC%E5%BE%97%C2%B7%E6%9E%97%E5%A5%87/"><span class="tag">彼得·林奇</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BD%BC%E5%BE%97%C2%B7%E8%92%82%E5%B0%94/"><span class="tag">彼得·蒂尔</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%80%BB%E7%BB%93/"><span class="tag">总结</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%8C%E5%8D%A1/"><span class="tag">斌卡</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9B%B9%E5%A4%A9%E5%85%83/"><span class="tag">曹天元</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%89%E8%B5%9E/"><span class="tag">有赞</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9D%8E%E7%AC%91%E6%9D%A5/"><span class="tag">李笑来</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%A7%91%E6%99%AE/"><span class="tag">科普</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%8F%E5%85%B8/"><span class="tag">经典</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%8F%E6%B5%8E/"><span class="tag">经济</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%97%E4%BC%AF%E7%89%B9%E3%83%BBT%E3%83%BB%E6%B8%85%E5%B4%8E/"><span class="tag">罗伯特・T・清崎</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B4%A2%E5%AF%8C/"><span class="tag">财富</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B4%BE%E6%A3%AE%C2%B7%E5%BC%97%E9%87%8C%E5%BE%B7/"><span class="tag">贾森·弗里德</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%92%B1%E7%A9%86/"><span class="tag">钱穆</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%98%BF%E5%88%A9%E6%96%AF%E6%B3%B0%E5%B0%94%C2%B7%E5%85%8B%E7%BD%97%E5%B0%94/"><span class="tag">阿利斯泰尔·克罗尔</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="jerrychen的博客" height="28"></a><p class="is-size-7"><span>&copy; 2022 jerrychen</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users, <span id="busuanzi_value_site_pv">0</span>&nbsp;visits</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>