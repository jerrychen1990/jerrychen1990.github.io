<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: nlp - jerrychen的博客</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="jerrychen的博客"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="jerrychen的博客"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="jerrychen的博客"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="jerrychen的博客"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="jerrychen"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"jerrychen的博客","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"jerrychen"},"publisher":{"@type":"Organization","name":"jerrychen的博客","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.15.2/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?e7c8dc4aeff2a6cff0b51140680831cd";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="jerrychen的博客" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li class="is-active"><a href="#" aria-current="page">nlp</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-09-28T16:00:00.000Z" title="9/29/2021, 12:00:00 AM">2021-09-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-01-05T09:42:42.389Z" title="1/5/2022, 5:42:42 PM">2022-01-05</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">5 minutes read (About 704 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/29/nlp/VOLT/">《Vocabulary Learning via Optimal Transport for Neural Machine Translation》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.15671">论文链接</a></p>
<h2 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h2><p>在机器翻译的任务中，合理选择词表和词表的大小至关重要。论文基于Marginal Utility（边际效益）这一经济学概念，提出通过最大化的Marginal Utiltiy of Vocabularization（下文简称MUV）的方式来优化下游任务。关于优化MUV的方法，又有搜索求解和VOLT（转化为Optimal Transport问题）两种方式，后者在效果接近的前提下大大节省计算量，更加低碳</p></div><a class="article-more button is-small is-size-7" href="/2021/09/29/nlp/VOLT/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-04-14T16:00:00.000Z" title="4/15/2021, 12:00:00 AM">2021-04-15</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-01-05T09:42:42.389Z" title="1/5/2022, 5:42:42 PM">2022-01-05</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">12 minutes read (About 1808 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/04/15/nlp/beyond_the_accuracy/">《Beyond Accuracy: Behavioral Testing of NLP Models with CheckList》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.04118">论文链接</a></p>
<p>这篇是ACL2020的最佳论文。论文指出现有的模型效果评估方案的问题，同时借鉴软件测试的方法，提出了一种全新的NLP模型测试方法（个人认为迁移到CV领域也不麻烦）CheckList。这种测试方案可以帮助人们更清晰、系统地了模型各个方面的优缺点。</p></div><a class="article-more button is-small is-size-7" href="/2021/04/15/nlp/beyond_the_accuracy/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-07-11T16:00:00.000Z" title="7/12/2020, 12:00:00 AM">2020-07-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-01-05T09:49:26.503Z" title="1/5/2022, 5:49:26 PM">2022-01-05</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">5 minutes read (About 812 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/07/12/nlp/tplinker/">《TPLinker:Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.13415">论文链接</a></p>
<h2 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h2><p>给定schema的SPO抽取：从文本中抽取去SPO(Subject-Predicate-Object)三元组。其中Predicate是事先定义好的关系，Subject和Object是文中的span<br><a name="jDZUi"></a></p>
<h2 id="TPLinker的特点"><a href="#TPLinker的特点" class="headerlink" title="TPLinker的特点"></a>TPLinker的特点</h2><ul>
<li>能够处理SEO(SingleEntityOverlap)和EPO(EntityPairOverlap)两种情形<ul>
<li>SEO：张三和李四都是北京人 -&gt; (张三,出生地,北京),(李四,出生地,北京)</li>
<li>EPO：江苏的省会是南京 -&gt; (江苏,包含,南京),(江苏,省会,南京)</li>
</ul>
</li>
<li>Single-stage的方案，原始文本过一次Encoder之后，便可以解码得到整个spo三元组</div><a class="article-more button is-small is-size-7" href="/2020/07/12/nlp/tplinker/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-07-01T16:00:00.000Z" title="7/2/2020, 12:00:00 AM">2020-07-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-01-05T09:42:42.389Z" title="1/5/2022, 5:42:42 PM">2022-01-05</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">2 minutes read (About 300 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/07/02/nlp/neural_openie/">《Neural Open Information Extraction》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1805.04270.pdf">论文链接</a></p>
<ul>
<li>目标：从输入文本中抽取schema-free的spo三元组</li>
<li>模型：<ul>
<li>encoder-decoder的seq2seq模型</li>
<li>原文输入encoder，得到一个encoded embedding</li>
<li>目标序列格式为<arg1>subject</arg1><rel>predication</rel><arg2>object</arg2></li>
<li>引入copy机制，从生成的token和copy的token中选择一个</li>
<li>architecture:<img src="/images/neural_openie.png" alt="architecture"></li>
</ul>
</li>
<li>实验：<ul>
<li>数据<ul>
<li>训练数据从wikipedia的dump构建，36,247,584 pairs,地址：<a target="_blank" rel="noopener" href="https://1drv.ms/u/s!ApPZx_TWwibImHl49ZBwxOU0ktHv">https://1drv.ms/u/s!ApPZx_TWwibImHl49ZBwxOU0ktHv</a></li>
<li>测试数据：3200 sentence with 10369 extractions <a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/D16-1252.pdf">https://www.aclweb.org/anthology/D16-1252.pdf</a></li>
<li>比较对象：OpenIE4(一个基于规则的提取器)</li>
<li>结果：更高的AUC</div><a class="article-more button is-small is-size-7" href="/2020/07/02/nlp/neural_openie/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-07-01T16:00:00.000Z" title="7/2/2020, 12:00:00 AM">2020-07-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-12-20T07:40:08.499Z" title="12/20/2021, 3:40:08 PM">2021-12-20</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">2 minutes read (About 368 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/07/02/nlp/supervised-openie/">《Supervised Open Information Extraction》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/N18-1081.pdf">论文链接</a></p>
<ul>
<li>目标：构建一个基于监督学习的openie</li>
<li>建模方式：sequence-labeling</li>
<li>输入：token序列+token的POS信息+基于SRL的predicate开头token的信息</li>
<li>输出：BIO方式标注的predicate ARG0 ARG1 ARG2标签<ul>
<li>ARG0表示subject ARG1表示object ARG2表示spo的附加条件（比如时间、地点、情景等）</li>
<li>这里的object定义比较灵活，可以不是一个实体</li>
<li>每个token输出一个probability，span的probability由包含的所有token的probability相乘得到。作者验证相乘的方式是最好的计算span probability的方案</div><a class="article-more button is-small is-size-7" href="/2020/07/02/nlp/supervised-openie/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-03-09T16:00:00.000Z" title="3/10/2020, 12:00:00 AM">2020-03-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-12-20T07:40:57.905Z" title="12/20/2021, 3:40:57 PM">2021-12-20</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">2 minutes read (About 318 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/03/10/nlp/TextFromKG/">《Text Generation from Knowledge Graphs with Graph Transformers》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.02342.pdf">论文链接</a></p>
<ul>
<li>解决问题：给定一篇论文的title +通过知识抽取工具从论文abstract里抽取出的知识图谱，用生成式模型生成文章的abstract</li>
<li>整体框架：用BiLSTM encode title, 用Graph Transformer encode 知识图谱。 decode过程中同时可以attention到title和图谱的encode特征，同时加上copy机制</li>
<li>细节：graph transformer用节点相邻的节点作为该节点的context，其他和text transformer类似</li>
<li>细节：原始图谱的边是有标签且无向的，在做graph transformer之前，将原始图的边改造成两个节点，这样得到的图的边是有向、无标签的。 同时加上一个global的节点，和所有节点都连接，让整个图联通</li>
<li>数据集：自建数据集，包含40k论文</li>
<li>评价指标：人工打分+BLUE+METEOR</div><a class="article-more button is-small is-size-7" href="/2020/03/10/nlp/TextFromKG/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-01-16T16:00:00.000Z" title="1/17/2020, 12:00:00 AM">2020-01-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-12-20T07:44:00.121Z" title="12/20/2021, 3:44:00 PM">2021-12-20</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">2 minutes read (About 358 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/01/17/nlp/survey-OPENIE/">《A Survey on Open Information Extraction》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.05599.pdf">论文链接</a></p>
<ul>
<li>information extraction从文本中抽取出SPO三元组,传统的information extraction都是抽取事先给定的关系</li>
<li>Open information extraction(Open IE)的关系无需实现给定，能够自动从大量的文本中发掘出关系(关系可能是原文中的span，也可能不是)</li>
<li>OPEN IE的三个挑战：<ul>
<li>Automation：需要手动标注的数据必须限制在较小的数量级</li>
<li>Corpus Heterogeneity:能在不同分布的数据集上work，不能依赖领域相关的信息，比如NER。只能用POS这些浅层tag</li>
<li>Efficiency：需要在大量数据上运行，需要预测性能高，只能依赖POStag这些浅层信息</li>
</ul>
</li>
<li>OPEN IE的方法：<ul>
<li>Learning-based System: TEXTRUNNER/WOE/OLLIE</li>
<li>Rule-based System:利用语言学、统计学特征+规则 PredPatt</li>
<li>Clause-based System: Stanford OpenIE</li>
<li>Systems Capturing Inter-Proposition Relationships: 同时抽取三元组以及原文中三元组成立的前提</div><a class="article-more button is-small is-size-7" href="/2020/01/17/nlp/survey-OPENIE/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-11-18T16:00:00.000Z" title="11/19/2019, 12:00:00 AM">2019-11-19</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-12-20T07:46:34.929Z" title="12/20/2021, 3:46:34 PM">2021-12-20</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">2 minutes read (About 272 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/11/19/nlp/KG-BERT/">《KG-BERT: BERT for Knowledge Graph Completion》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.03193.pdf">论文链接</a></p>
<ul>
<li>目标：做KG-completion</li>
<li>思路：基于预训练的BERT，做SPO三元组的embedding， 不依赖原句</li>
<li>训练<ul>
<li>将SPO三元组拼接成[CLS]S[SEP]P[SEP]O[SEP]的形式， 其中S和O用同样的segment embedding</li>
<li>S和O是entity的name或者description</li>
<li>用二分类判断三元组是否正确， 或者用多分类，给定S\O 判断relation的类型</li>
</ul>
</li>
<li>预测<ul>
<li>给定SPO，判断是否正确</li>
<li>给定SO，判断relation在schema中的哪一个</li>
<li>给定 SP，判断O是哪个（将所有可能的O列举，拼接成三元组之后预测），按照得分排序取第一个</div><a class="article-more button is-small is-size-7" href="/2019/11/19/nlp/KG-BERT/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-11-06T16:00:00.000Z" title="11/7/2019, 12:00:00 AM">2019-11-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-12-20T07:31:48.339Z" title="12/20/2021, 3:31:48 PM">2021-12-20</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">4 minutes read (About 545 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/11/07/nlp/GPT/">《Improving Language Understanding by Generative Pre-Training》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">论文链接</a></p>
<ul>
<li>目标：用与训练的LM模型提升NLU任务的效果</li>
<li>基于大量未标注语料训练的两个问题<ul>
<li>如何设置合理的训练目标？LM/NMT/discourse coherence? 这也是GPT和bert的区别之一</li>
<li>如何将预训练的模型得到的表征应用到下游任务中去？</li>
</ul>
</li>
<li>模型结构<ul>
<li>一个没有encoder-attention的transformer decoder</li>
<li>给定一个窗口的输入(最后n个token)，预测下一个单词</li>
<li>模型返回的是最后一个token对应的embedding，而不是真个窗口sequence的embedding</div><a class="article-more button is-small is-size-7" href="/2019/11/07/nlp/GPT/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-11-06T16:00:00.000Z" title="11/7/2019, 12:00:00 AM">2019-11-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-12-20T07:37:16.906Z" title="12/20/2021, 3:37:16 PM">2021-12-20</time></span><span class="level-item"><a class="link-muted" href="/categories/nlp/">nlp</a></span><span class="level-item">4 minutes read (About 608 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/11/07/nlp/survey-NER/">《A Survey on Deep Learning forNamed Entity Recognition》</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.09449.pdf">论文链接</a></p>
<ul>
<li>NER是信息抽取、问答系统、机器翻译的一项基础工作，DNN的应用让NER任务有了长足的进步</li>
<li>NER分为两类coarse-grained NER:比较粗粒度的划分entity，比如通用NER。 fine-grained NER：更加细分的实体类型，通常是和具体的业务相关的实体，一个mention可以属于多个实体类别</li>
<li>数据集：见原文table1。 比较常用的有：<ul>
<li>OntoNotes：18 coarse entity type consisting of 89 subtype</li>
<li>CoNLL03 4 entity types</li>
</ul>
</li>
<li>工具：见原文table2 StanfordCoreNLP/NLTK/spaCy</li>
<li>评价指标：<ul>
<li>exact-match evaluation： 用全匹配方法计算F1。会有些偏严，指标偏低</li>
<li>relaxed-match evaluation: 宽松匹配方案，不太好控制</div><a class="article-more button is-small is-size-7" href="/2019/11/07/nlp/survey-NER/#more">Read more</a></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/categories/nlp/page/0/">Previous</a></div><div class="pagination-next"><a href="/categories/nlp/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/categories/nlp/">1</a></li><li><a class="pagination-link" href="/categories/nlp/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Jerrychen"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Jerrychen</p><p class="is-size-6 is-block">Developer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hanzhou/Zhejiang</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">32</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">9</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">37</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/jerrychen1990" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/jerrychen1990"><i class="fab fa-github"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/jerrychen1990" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/nlp/"><span class="level-start"><span class="level-item">nlp</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%8E%86%E5%8F%B2/"><span class="level-start"><span class="level-item">历史</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%95%86%E4%B8%9A/"><span class="level-start"><span class="level-item">商业</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"><span class="level-start"><span class="level-item">大数据</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%89%A9%E7%90%86/"><span class="level-start"><span class="level-item">物理</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%94%9F%E7%89%A9/"><span class="level-start"><span class="level-item">生物</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"><span class="level-start"><span class="level-item">计算机</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%A4%E7%9F%A5/"><span class="level-start"><span class="level-item">认知</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E9%87%91%E8%9E%8D/"><span class="level-start"><span class="level-item">金融</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-11-23T16:00:00.000Z">2021-11-24</time></p><p class="title"><a href="/2021/11/24/physics/%E4%B8%8A%E5%B8%9D%E6%8E%B7%E9%AA%B0%E5%AD%90%E4%B9%88/">《上帝掷骰子么》书摘</a></p><p class="categories"><a href="/categories/%E7%89%A9%E7%90%86/">物理</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-11-19T16:00:00.000Z">2021-11-20</time></p><p class="title"><a href="/2021/11/20/computer/LSH/">局部敏感哈希(LSH)与文本去重</a></p><p class="categories"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/">计算机</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-09-28T16:00:00.000Z">2021-09-29</time></p><p class="title"><a href="/2021/09/29/nlp/VOLT/">《Vocabulary Learning via Optimal Transport for Neural Machine Translation》</a></p><p class="categories"><a href="/categories/nlp/">nlp</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-14T16:00:00.000Z">2021-04-15</time></p><p class="title"><a href="/2021/04/15/nlp/beyond_the_accuracy/">《Beyond Accuracy: Behavioral Testing of NLP Models with CheckList》</a></p><p class="categories"><a href="/categories/nlp/">nlp</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-19T16:00:00.000Z">2021-01-20</time></p><p class="title"><a href="/2021/01/20/acknowledge/%E9%87%8D%E6%9D%A52/">《重来2》书摘</a></p><p class="categories"><a href="/categories/%E8%AE%A4%E7%9F%A5/">认知</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">November 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">September 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/04/"><span class="level-start"><span class="level-item">April 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">January 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/07/"><span class="level-start"><span class="level-item">July 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/04/"><span class="level-start"><span class="level-item">April 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/03/"><span class="level-start"><span class="level-item">March 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/01/"><span class="level-start"><span class="level-item">January 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">November 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">October 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">September 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">August 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">May 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/02/"><span class="level-start"><span class="level-item">February 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/01/"><span class="level-start"><span class="level-item">January 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/10/"><span class="level-start"><span class="level-item">October 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/05/"><span class="level-start"><span class="level-item">May 2017</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/02/"><span class="level-start"><span class="level-item">February 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/01/"><span class="level-start"><span class="level-item">January 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2016/04/"><span class="level-start"><span class="level-item">April 2016</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2016/03/"><span class="level-start"><span class="level-item">March 2016</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2015/09/"><span class="level-start"><span class="level-item">September 2015</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/BERT/"><span class="tag">BERT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Best-Paper/"><span class="tag">Best Paper</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FIRE/"><span class="tag">FIRE</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GPT/"><span class="tag">GPT</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/IE/"><span class="tag">IE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KG/"><span class="tag">KG</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NER/"><span class="tag">NER</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLG/"><span class="tag">NLG</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLU/"><span class="tag">NLU</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NMT/"><span class="tag">NMT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenIE/"><span class="tag">OpenIE</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Recommendation/"><span class="tag">Recommendation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/best-paper/"><span class="tag">best paper</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/kylin/"><span class="tag">kylin</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/self-supervised-Learning/"><span class="tag">self-supervised Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/survey/"><span class="tag">survey</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B9%A6%E6%91%98/"><span class="tag">书摘</span><span class="tag">15</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%81%A5%E8%BA%AB/"><span class="tag">健身</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%87%AF%E5%88%A9%C2%B7%E9%BA%A6%E6%A0%BC%E5%B0%BC%E6%A0%BC%E5%B0%94/"><span class="tag">凯利·麦格尼格尔</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%98%E6%9C%AA%E9%B9%8F/"><span class="tag">刘未鹏</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%90%B4%E6%99%93%E6%B3%A2/"><span class="tag">吴晓波</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BD%BC%E5%BE%97%C2%B7%E6%9E%97%E5%A5%87/"><span class="tag">彼得·林奇</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BD%BC%E5%BE%97%C2%B7%E8%92%82%E5%B0%94/"><span class="tag">彼得·蒂尔</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%80%BB%E7%BB%93/"><span class="tag">总结</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%8C%E5%8D%A1/"><span class="tag">斌卡</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9B%B9%E5%A4%A9%E5%85%83/"><span class="tag">曹天元</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%89%E8%B5%9E/"><span class="tag">有赞</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9D%8E%E7%AC%91%E6%9D%A5/"><span class="tag">李笑来</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%A7%91%E6%99%AE/"><span class="tag">科普</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%8F%E5%85%B8/"><span class="tag">经典</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%8F%E6%B5%8E/"><span class="tag">经济</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%97%E4%BC%AF%E7%89%B9%E3%83%BBT%E3%83%BB%E6%B8%85%E5%B4%8E/"><span class="tag">罗伯特・T・清崎</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B4%A2%E5%AF%8C/"><span class="tag">财富</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B4%BE%E6%A3%AE%C2%B7%E5%BC%97%E9%87%8C%E5%BE%B7/"><span class="tag">贾森·弗里德</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%92%B1%E7%A9%86/"><span class="tag">钱穆</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%98%BF%E5%88%A9%E6%96%AF%E6%B3%B0%E5%B0%94%C2%B7%E5%85%8B%E7%BD%97%E5%B0%94/"><span class="tag">阿利斯泰尔·克罗尔</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="jerrychen的博客" height="28"></a><p class="is-size-7"><span>&copy; 2022 jerrychen</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users, <span id="busuanzi_value_site_pv">0</span>&nbsp;visits</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>