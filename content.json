{"pages":[],"posts":[{"title":"《暗时间》书摘","text":"豆瓣链接 贝叶斯 先验概率（模型的本身可能的概率， 奥卡姆剃刀） 后验概率（模型生成样本的概率，最大化后验概率就是极大似然估计） 模型泛化（过拟合、欠拟合也就是在先验概率、后验概率平衡） 没有先验概率的时候，假设虽有模型概率一致，这就用到最大似然估计 贝叶斯+奥卡姆剃刀（不考虑先验概率） 听起来还是个极大似然估计（后验概率）的问题 信息论+贝叶斯： 一个模型的好坏取决于模型的编码长度lg(先验概率)+这个模型下数据的编码长度lg(后验概率) 强化学习是一种复杂模型（模型编码长度长）？ 还是一个简单模型，用多个步骤来求解呢？ 康德尔对角线 引申出不完备定理、图灵停机问题、Y算子、罗素悖论、说谎砍头悖论 一个理论内的形式符号，总有那么一个正确但是不能在这个理论范围内验证的理论 打破了希尔伯特的形式数学大厦 哥德巴赫猜想可能就是这么一个数论理论下的不可验证理论 最大熵 每次获取最大的信息量的原理 二分查找、快拍， 找12个球里面的较轻／较重的一个 每次比较、判断的时候尽可能排除掉最多的选项（信息最大） 这样的算法最差情况（下界最佳），是否全局期望时间也最短（是的， 期望计算公式，平均运行时间说明这一点） 一点注记:有人可能会疑惑，难道我们人类也是基于这些天真的假设来进行推理的?不是的。 事实上，统计机器学习方法所统计的东西往往处于相当表层(shallow)的层面，在这个层 面机器学习只能看到一些非常表面的现象，有一点科学研究的理念的人都知道:越是往表层 去，世界就越是繁复多变。从机器学习的角度来说，特征(feature)就越多，成百上千维度 都是可能的。特征一多，好了，高维诅咒就产生了，数据就稀疏得要命，不够用了。而我们人类的观察水平显然比机器学习的观察水平要更深入一些，为了避免数据稀疏我们不断地发明各种装置(最典型就是显微镜)，来帮助我们直接深入到更深层的事物层面去观察更本质的联系，而不是在浅层对表面现象作统计归纳。举一个简单的例子，通过对大规模语料库的统计，机器学习可能会发现这样一个规律:所有的“他”都是不会穿 bra 的，所有的“她” 则都是穿的。然而，作为一个男人，却完全无需进行任何统计学习，因为深层的规律就决定了我们根本不会去穿 bra 。至于机器学习能不能完成后者(像人类那样的)这个推理，则 是人工智能领域的经典问题。至少在那之前，声称统计学习方法能够终结科学研究(原文) 的说法是纯粹外行人说的话。","link":"/2019/01/28/acknowledge/%E6%9A%97%E6%97%B6%E9%97%B4/"},{"title":"《把时间当做朋友》书摘","text":"豆瓣链接 近期，在朋友的推荐下，读了《把时间当作朋友》这本书。确是一本很好的时间管理，不，应该是自身管理类别的书。这一类的书我读过并不多，相比于《少有人走的路》，这本书更有实践指导意义一些。作者写书逻辑很严密，比较对我这种理工男的胃口。书中提到，长时间的积累会让你在多年后发现有一个质的改变。读书，自然也是一种积累。最简单的方式就是摘录其中的句子了，先采用这种最朴素的方式开始吧。下面是我摘录书中的句子，有的是原句，有的是按照自己的理解复述的句子。 不要用手写记录笔记，用打字更快，留出实践思考 神奇的心智，很多人都还没有开启 不要担心学了没有用，just learn it 不要对未知有太多的恐惧和钻牛角尖，有时候放过未知先向前更重要 践行上一点的时候记得把未知的东西记录下来，可以回看 不要追求完美 我想要的可以用我有的换来，当我有的越来越多，我想要的就更加容易获得了 注重积累的力量 不要做着工作，心里想着梦想。这样你在两方面都没有积累，没有进步 速成绝无可能 完美永不存在 未知永远存在 现状无法马上摆脱 做任何事之前先估算其熟悉程度，在估算时间 那些能够做对、做好的人绝不轻易打击别人，因为他们知道这一路过来是多么的不容易 喜欢做某件事只是因为那件事对你来说简单，容易受到奖励而已 运用事件－结果法记日志，不记感想 人一过30岁，能自己支配的时间就越来越少了 一些简单的工作，不具备现在做计划的条件的工作，不妨找一个看着绝对不会错的方向，先do it 再计划 在评判重要紧急的时候记得一条：先支付自己 沟通的好时间一般是上班时间的前30min，一般别人都是准时上班的，而且这个时间段也没有被安排什么事情 缺少验收机制会让你对你团队的工作结果非常失望 学习是投资回报最高的行为 获取知识的手段：体验、试错、观察、阅读、 正确的思考 捍卫有些传统，可能只是为了报复————自己曾经被那些传统伤害过（5只猴子的故事） 我国的教育过度地把文字和文学关联在了一起 避免选择性输入 不知道“目标”与“计划”的区别的人常常会因为死守计划，偏离了目标（项目管理适用） 不知道“政府”与“国家”的区别的人通常难以沟通 很多现象非常复杂，不能用因果原因概括。 比如墨菲定律“坏的事情总会发生”(因果互相影响) 人们对于喜悦通常会停下手头的事情享受，对于恐惧尝尝会立刻采取行动避免，而采取的行动往往事与愿违:一个原因应为对结果的恐惧产生了，最终导致了结果的发生。 要让别人相信你，证明的责任在你。别人没有证明你的论点是错误的的责任。 思考某个现象的时候，考虑考虑它的对立解释 交流的成败很大程度上取决于听者而非说者 大脑的速度快于说的速度，这让我们习惯性的走神 人们对于重点的部分反复说，反而让这部分信息到了听者那端碎片化 大脑有自动拼接功能，对于错过了的、碎片化的信息，想当然地拼接，很可能就导致“听错了” 不要过早质疑，克制住质疑的想法留到最后。 由于大脑比说话快，在倾听的过程中，我们可以用多余的脑力做回顾与预期 知道什么话该说，什么话不该说很重要，通过长时间的努力，让自己成为能说更多话的人更重要。 知无不言，言无不尽在分享知识的时候是有效的，但在日常生活中，这样的方式却是很不好使的 注意观察自己的话有没有人听 可言而不言，失人。 不可言而言，失言 每一代人的出生，都以看作是野蛮对文明的侵略，我们必须在积重难返之前教化他们————关于每个人天生会有的“自以为是” 类比是跨越未知与已知的重要方法，这就是读“杂书”的重要性，因为在接受新的事物的时候，你有足够多的知识帮助你用来类比。 不是有兴趣才能做好，是做好了才能有兴趣（其实这也有点鸡生蛋、蛋生鸡）的意思。 所有成功，本质上都是策略与坚持，而坚持本身就是最好的策略 大脑有主动遗忘痛苦的功能，这使得我们喜欢怀旧（只记得往日的美好，忘记往日的痛苦） 然而这个机制并不是很利于我们进步，有时候我们得控制痛苦的情绪，或者记录下来，以供勉励 有些幸福建立在比较上，然而也有很大一部分不是，我们应该追求不建立在比较之上的幸福。或者我们选择和昨天的自己比较 千万不要相信，机不可失、失不再来。这会让我们过于急躁 （影响力中利用紧迫制造影响） 对于准备好的人，好运终会来的，运气只留给准备好的人。很多骗术都通过“机不可失，失不再来”来给你带来心理压力，迫使你做出不理智的选择。 所谓的积累人脉，首先要提高自己的价值，第二要学会用公平交易提升自己 停止嘲弄他人，嘲弄他人为乐证明自己这方面“强”，然而正真的“强”是不需要证明的 有时候比改变缺点更容易的是，漠视自己的优点，这样可以使自己更加“低调一点”，让别人眼里的你和真实的你产生正向误差。 灵感这东西跟钱一样，是攒出来的，不是想出来的 鸡尾酒现象：在鸡尾酒会中，对于离得比较远的人的谈话，我们是自动过滤的，然而，如果谈话的内容包含自己的名字，我们却很容易捕捉到，这是一种心理学现象，带着关注点看书，让你很容易捕捉到你需要的知识 能否坚持做一些自己认为重要但是很无趣的事情，标志着你是否已经心智成熟，掌控自己的大脑。 黄金分割法则很重要，单纯的把所有时间都做一件事，是不正常的现象。花费61.8%的自由时间做你觉得重要的事。 愿上帝赐我从容去接受我不能改变的,赐我勇气去改变我能改变的,赐我智慧去分辨这两者的区别","link":"/2017/05/17/acknowledge/%E5%81%9A%E6%97%B6%E9%97%B4%E7%9A%84%E6%9C%8B%E5%8F%8B/"},{"title":"《财富自由之路》书摘","text":"豆瓣链接 并不像书名写的那么直接，没有太多关于财富自由的具体计划，确有更多元认知上的提升，财富自由只是提高认知路上的一个不经意间到达的里程碑 判断一个人执行力强大与否，就是看他做的不好的时候能否坚持继续去做 在自己的舒适区，大家都能执行好。在尚未舒适的领域，能够埋头坚持做下去才是真正的执行力 一个人的发音与听力往往是没有直接联系的 印度人英语发音不准却能顺畅地听懂英语，只会说方言的人也能顺畅地听懂普通话。 当你专注时，时间会飞快流逝。所以专注时度过那个笨拙的起步过程的最好方法 学习一个新技能的时候，总归会经历痛苦的起步过程，在这个过程中你做得很挣扎，效果却不好，甚至会引来嘲笑。专注就是快速度过这个阶段的最好方法 每个人都有”商业模式” 一份时间出售一次：朝九晚五（有个很低的上限） 一份时间出售多次：作者/内容创作 购买别人的时间再出售：企业家、投资者 将时间和财富作为两种资源来看待。每个人的时间是一样多的，如果只能出售一次，时薪再高也有个明显的天花板。只能作为积累初始财富的一种方式，最终一定要抛弃这种线性增长的财富获取方式 你见过多少人因为磨洋工和频繁跳槽获得财富自由了？你的薪水只是你的估值，关注你的自身价值更重要 上班划水（减少时间）、跳槽（增加自己的估值）增加的是你单位时间的价格。重要的是成长（提高自己的价值）以及升级“商业模式” 有能力的人最终都会被低估了，当自己被过分低估的时候，就是自己该出来闯荡一番的时候 和彼得原理有些相像。不断成长，不爽的时候就有底气说走就走 每天投资自己的时间有多少？怎么投资的，投资到哪些方向？ 给自己打工的同时给老板打工 将公司的需求和自身成长的需求结合起来，你工作的时间就被售卖了两遍（或者说同时提供了产出和产能）。这是一个非常棒的状态，找工作和沟通岗位的时候一定要确保做到这一点 注意力（流量）就是一种资源，有很多买卖注意力的商业模式（广告）。管理好自己的注意力就是管理好自己的资产能够长时间集中注意力几乎是所有学习能力强的人的必备标志 把注意力放在能够带来自身成长的地方 《注意力》 冥想是一种提高注意力的方式 几乎所有进步都是在放弃了部分安全感的情况下获得的不要和缺乏安全感的人合作（他们不会从内心里信任你） 安全感和控制欲相关联 过分的安全感体现在强调事情的确定性、过多关注风险而没有通过概率学的角度分析期望。德扑是一个检测安全感的游戏 活在未来 需要拓宽视野，对未来有个准确的预测 未来的世界是什么样的？（采集信息） 未来的自己希望是什么样的？ 需要做哪些准备？ 准备（活在未来） 你自己得是一个贵人，才能遇到更多贵人 有效社交（管理注意力的一种）。真正有效的社交是双赢的 不要做表现型人格的人， 要做成长型人格 表现型人格: 在意自己当下的表现 成长型人格: 在意自己未来的表现 真正的安全感来自于对于未来深刻全面的思考你觉得别人是在冒险，可能别人是做了深度思考之后充分分析了风险之后的行动 对于我们不懂的事情总觉得风险很大，真正研究了的人能看见确定性（比特币？） 抱怨会让一个人变得令人厌倦和讨厌 要了解一个人，看他的书单不失为一种好方法 成功无非是解答题高手作对了选择题 要有判断的能力、要有执行力，缺一不可 每次跨界就是给自己扩展一个新的维度（多维度人才才有竞争力）价格决定于供需，做被需要的人换位思考，不是和某个人换，而是和整个世界换。考虑世界上需要什么样的人 我需要的跨界 计算机+（金融、脑科学、心理学…） 镜像神经元学习的一个方法：物理上尽可能接近成功的目标 看到别人的某项行为，自己能有相同的感受，心理学重要的依据 当你想学习某项技能时，给他赋予一些重大的意义，让自己根本没法停下来“坚持”、”努力” 都太被动、痛苦 在重复足够多次数（熟练到神经元内化）之前，很多人就放弃了，从而体会不到那种神经元内化之后熟练的快感 学习也是个指数增长的曲线。依靠赋予重大意义等提高主观能动性的方式度过开始的平缓期，后续就很顺畅了 情绪是理智的快捷方式、直觉是情绪的快捷方式 财富自由只是一个里程碑，后面还有更长的路要走，不能和家人一起成长就很难走下去 避免独善其身的想法，要和家人一起成长才能走得更远 一个人可以走得很快，但一群人才能走得很远","link":"/2017/10/13/acknowledge/%E8%B4%A2%E5%AF%8C%E8%87%AA%E7%94%B1%E4%B9%8B%E8%B7%AF/"},{"title":"《自控力》书摘","text":"豆瓣链接 自控力挑战 我不要：戒掉一个坏习惯 我要做：养成规律的作息 我想要：成为自由职业者 承认自己会失控。了解什么情况下、什么原因会失控才是关键 自控力的神经学原理：前额皮质 原始本能：人类进化初期积累的本能 自控能力：前额皮质控制的能力：我不要、我要、我想要 两者对抗，有时也合作，不一定要遏制原始本能，更好的利用原始本能有时事半功倍 有意识的做决定才能用到自控力，当你想着别的事情的时候，你的决定就是本能的、最简单的那个选择 实践： 记录一下这一天，你是怎么屈服于冲动的，是不是无意识下就按本能作了。如果你注意到了这点，运用你的意志力，还会屈服于冲动么？ 大脑需要经常锻炼，在家里给自己设置一些挑战（购买薯片放在显见的位置）等，可以帮助大脑锻炼（我不要）的能力 冥想，像锻炼肌肉一样锻炼大脑 练习放慢呼吸，可以提高自控力（这是一种生理上的力量） 每天一个短暂的锻炼以及充足的睡眠都是提升自控力很好的方案 过强的意志力会消耗太多能量，会想慢性疾病一样伤害你的身体。 所以适当的放松（不是放纵。只是不去想眼前的压力）能够让你的身体得到恢复。 每天一个小锻炼（比如收拾屋子） 减少行为的变化性，这样可以防止失败 取消许可，更加关注目标。不要让善行变成自己做坏事的”许可证”（在达成自控目标之后过分放纵） 为自己的“我想要”目标设置分泌多巴胺的奖励机制，让自己更有动力去做 在收到欲望的诱惑堕落之后，好好感受，在堕落的过程中是否真的获得了幸福，抑或只是收到了多巴胺的奖励机制影响，觉得自己获得了幸福 最有效的解压方法包括：锻炼或参加体育活动、祈祷或参加宗教活动、阅读、听音乐、与家人朋友相处、按摩、外出散步、冥想或做瑜伽，以及培养有创意的爱好。 最没效果的缓解压力的方法则包括：赌博、购物、抽烟、喝酒、暴饮暴食、玩游戏、上网、花两小时以上看电视或电影。 在遇到挫折，没有完成自控力计划的时候，不要太严苛的对自己。想象没有完成计划的是自己的朋友，自己应该怎么安慰这个“他” 我是一个很容易模仿周围人行为的人，这就让我的交友需要更加精心挑选 完全控制自己的想法是不可能的， 我们能控制的是自己信什么、做什么。所以有不好的想法的时候，想一想白熊实验（越想着不要想某件事，其实是把某件事又重复加深了一遍），不要刻意压制这个想法 自控力的终极奥义：提高注意力，让多个不同的“自己”融合在一起，有意识的做出决定，而不是让大脑自行其是 两种威胁 一种来自外部（比如过饿），身体会有应激反应，不会调动前额皮质就能做出正确的决定 第二种来自自己的本能（甜甜圈）。需要前额皮质，来对抗这种威胁 压力是自控力的天敌，当你处于压力中的时候，往往更容易失去自控力 意志力和肌肉力量一样是有限的， 人的一天中意志力往往会逐渐下降 夜食症（的解释）。。。 在晚上往往容易放纵 这也是早睡早起的一大好处 找到自己意志力最好的时刻，用来处理最重要的事情 健康的饮食对意志力的锻炼非常有好处 锻炼意志力 设定一些小目标（我想要、我不要。。。）哪怕和你的目标无关，通过锻炼，意志力得到提升，你在其他方面也会表现的更好 意志力是有限的，但是意志力往往比我们自身想象的强。在你觉得意志力耗尽的时候，想象完成挑战后带来的好处，挺过去，你的意志力就得到了锻炼（锻炼肌肉一样的道理） 善行之后的恶行 通常人们在作了“善事”，或者觉得自己作了“善事”之后，渴望对自己奖赏。将一些想要做的诱惑，变成了必须做的诱惑的事儿。导致善行之后的恶行 在抵抗了诱惑之后，面对新的诱惑，不要想“我已经成功抵抗了一个诱惑，可以放松一下了”。而是要想象上次是怎么抵抗诱惑的 “明天会更好的假象”很多意志力失败在于有今天放纵，明天我能控制住自己”的假象。 我们需要减少行为的变化性，认为自己的行为是一致一致的。今天的放纵的被赋予了一直都这么放纵的意义，这样就会从“明天会更好”的假象中解脱出来 一般99%的罪恶都伴随着1%的美好，让我们被那1%美好的自我感觉良好所蒙蔽 渴望与幸福 多巴胺，是一种大脑奖励承诺的分泌物，当分泌这种东西的时候，说明大脑判断做某件事能够让你有潜在的收获到奖励的机会，会促使人上瘾的一直做某件事。这个奖励（幸福）本身是两码事儿 神经营销学：通过一些让你分泌多巴胺的事情，来让你充满欲望，容易被诱惑。 比如气味营销（面包店门口的气味可能是专门调制出来吸引你的哦），试吃、打折、彩票广告等等 利用多巴胺：做一件很难的事情的时候，想象完成之后的美好场景、有利于分泌多巴胺，从而让你完成这件事 不确定的奖励（有小概率获得大奖励）比固定的的小奖励更能让人坚持下去 多巴胺的危险之处：他会驱使你去做获得奖励的事情，有时会忽略掉这个奖励是否真的给你带来快乐。 比如暴饮暴食垃圾食品，只是多巴胺告诉你“那样很快乐”，就会驱使你去做， 然而如果细细品尝食品，你会发现那并不是真的能带给你快乐，从而让你在“放纵”之后，存在一种失落感。 欲望不总是危险的，失去欲望的人生没有意义，我们要尝试让欲望为自己所用。 情绪与自控力 人在情绪低落的时候自控力较差，倾向于“奖励承诺”带来的多巴胺让自己快乐 恐惧管理：恐惧会让人更加倾向于排解压力，求助于“奖励承诺” 犯错时的自我苛责带来的罪恶感是让人堕落的一大原因。想象数次因为每天的目标没有完全完成而彻底放弃计划的自己吧 自我原谅：学会和自己相处，不必做自己的“严厉的家长”。因为我们的自控系统已经成熟，不再是小孩子。 在遇到挫折的时候自我原谅，不容易造成更大的堕落 “改变承诺” 幻想自己成功自控之后的情形，会让你充满动力和快乐，但是这和真正的做出改变还是不一样的，区分两者（对我太适用了） 乐观的悲观主义者更容易成功。 对未来报以乐观，避免自暴自弃。但是清醒的自我认知，知道自己什么时候更容易自控力失效，提前想好对策。 及时享乐的经济学 延迟快乐是人类的前额皮质才有的功能，对当下的奖励的应激反应是生物的本能 每个人心中都有一个折扣率， 对未来的价值会做一个打折 十分钟法则：在内心想要当下的奖励的时候，让自己等10分钟，并想着未来的利益，你还会屈从当下的冲动么 尽可能的从距离上减少当下的利益的诱惑， 多想象获得长期利益的结果，会有好处 预先承诺的价值：类似于破釜沉舟、高昂健身年费、给自己的游戏加密码等 我们容易将未来的自己理想化，好像和现在的自己不是同一个人， 向对待一个陌生人那样严苛的要求未来的自己 提高当下的你和未来的你的联系，更容易自控，不容易被诱惑控制 有时候也要避免好高骛远（太过于延迟满足）。 对于那些会随着时间流逝的快乐， 你真正追求的快乐需要把握当下 自控力会传染 每个人都有“镜像神经元” 这会让你容易受到别人的影响 人容易模仿正在接触的人都动作 人有移情的本能，能把别人的感受当做自己的感受 看见别人受到诱惑，自己更容易受诱惑 把意志力项目变成群体项目（抱团互相激励），有助于成功 群体认同，别人都这么做，很容易让你也这么做 “我不想”力量的局限（讽刺性反弹） 当你越让自己不要想某件事的时候，某件事越会一直浮现出来 - 我不要力量需要两个系统“操控”和“告警”，操控会指引你行动，“告警”则寻找你不想要的事物，然后发出告警。 “告警”系统是自动的，所以在你没有力量“操控”的时候，“告警”的力量就会让那个你不想要的东西挥之不去 避免讽刺性反弹的一个方法就是放弃自控。放弃不去想某件事，但是在行动上控制自己，就不会一直反弹了 为了避免讽刺性反弹，可以把“我不要”挑战转变成“我想要”挑战。“想要”的事情可以和“不要”的事情满足同样的欲望，但是不会带来伤害 驾驭冲动：将内省的冲动，“我不要”的欲望想象成浪花，自己是冲浪，而不是和他们搏斗 如果说真的有自控力秘诀，那么从科学的角度来说确实有一个，那就是集中注意力。当你作出决定的时候，你需要训练自己的大脑，让它意识到这一点，而不是让它自行其是。你需要意识到，你是如何允许自己拖延的，你是如何用善行来证明自我放纵是合理的。你也需要意识到，奖励的承诺并非总能兑现；未来的你不是超级英雄，也不是陌生人。你需要看清，自己身处的世界，无论是销售陷阱，还是社会认同，都在影响你的行为。当你的注意力即将分散的时候，或者你即将向诱惑投降的时候，你需要静下心来，弄清自己的欲望。你需要记住自己真正想要的是什么，什么才能真的让你更快乐。“自我意识”能帮你克服困难，实现最重要的目标。这就是我能想到的对“意志力”最恰当的定义","link":"/2019/09/16/acknowledge/%E8%87%AA%E6%8E%A7%E5%8A%9B/"},{"title":"《从0到1》书摘","text":"豆瓣链接个人对商业类别的书目并不太感兴趣， 这方面的知识一直时自己的薄弱环节。由于多位朋友推荐，近日来翻看了Peter·Thiel的《从0到1》。这本书还有个霸气十足的副标题——开启商业与未来的秘密。总体感觉：有一些新的认知上的启发，但对于实践上的指导意义却不太那么明显。有点类似于之前读的《富爸爸，穷爸爸》，《黑客与画家》这类书籍的感觉。美式风格十足，或许生活在美国，生活在哪个年代会更加感同身受一些吧。 照例，做一个简单的摘录： 每当我面试应聘者时，都会问这样的问题：“在什么重要问题上你与其他人有不同看法？” 这个问题到还真是在读这本书之前就有想过。越把我的年岁越往前推，这种与众不同的想法越多。年轻的我，对那些与我想法不同的“大多数人的想法”嗤之以鼻，固执的认为自己就是站在真理一边的“少数者”。比如： 上学的时候大家都喜欢多记住几个题型，期待考场上碰见自己做过的题。而我，从来不记题型，享受通过自己掌握的基础知识，探寻出解题方案的快感。 大家都对车辆的磕磕碰碰比较在意，刮擦一下就会把车停在路中间等待解决冲突。而我，觉得车是自由的象征，一些小刮擦不应该羁绊住你追逐自由的行程。只要不是影响车辆行动，大家各自道个歉，奔向各自的方向即可。多些碰撞印记反而还是阅历的象征。 我们这一代的成功标准大概是出任CEO，迎娶白富美什么的。而我，期待的是有个温暖的小屋，一家人围在火炉旁的交心畅谈。或者是深夜一盏孤灯下与一本好书的不期而遇。总之，对于商业上的成功不是那么感冒。以至读这本《从0到1》也只是蜻蜓点水。 也许还有过很多的与（离）众（经）不（叛）同（道）的想法，在年岁渐长的过程中都慢慢忘却了。到如今，已泯然众人矣。 现在，面试中如果被问到这个问题，可能我已无从回答。想来，正是那些你与他人不同的想法，才让你成为了你。才是你在作为Human类的众多子类之一时，亲手写出的override函数。 进步有两种方式：垂直的（科技的突破与创新），水平的（先进技术的推广与全球化） 作者喜欢用二维图表做展示，还是蛮直观的。我认同这个观点，垂直与水平的发展应该不是匀速的，该是如海浪般一波垂直、一波水平这样的方式。猜想如今处于上一波水平浪潮的尾端，下一波垂直浪潮正在到来。勇敢的迎接把，弄潮儿们！ 小企业谎言与垄断者谎言 在我们的印象中，各行各业都有那么一波势均力敌的企业在竞争，从而市场使得价格与价值趋于一致。其实，很多行业都处在一个垄断的情形中。垄断者会夸大其竞争对手的威胁，从而避免反垄断法带来的麻烦（垄断者谎言）。小企业们会夸大自己的市场占有率，从而博得在消费者心中的好印象（小企业谎言）。现实情况远比我们想象的2-8分化得多。其实要判断哪些企业其实是垄断型企业也很简单：那些市值很高、盈收很高的企业基本都是。在一个充分竞争的市场中， 几乎是无利可图的。创立企业的时候，与其在一个大的市场中和众人搏斗，远不如在一个较小的市场中，取得充分的垄断地位容易成功 后来居上的方法：占领小市场，扩大规模，破坏式创新 只要有足够的创新，后来者反而是有优势的。在这个时代，后来居上的机会要比过去的几百年大很多。 浅薄的人才会相信运气和境遇，强者只相信因果 一句被安利过很多次的鸡汤句。基本赞同，或许运气和境遇只是我们尚未发现规律的弱因果关系呢？ 金融其实是不明确思想的集中体现，因为只有人们不知如何赚钱时，才会想到去搞金融 所以金融都是泡沫么？《金融的解释》一书中解释金融是对社会资源优化配置的手段，是对社会生产有重要帮助的行为。一直没有搞清楚过金融的原理，难道是因为它是不明确思想的集中体现？ 罗尔斯的《正义论》以著名的“无知之幕”开篇。。。 在罗辑思维中听过“无知之幕”的故事：人类在决定一个政治体制，或者更广义上的组织结构时，要用一块“幕布”挡着。即没有人知道自己会在这样的组织结构中扮演什么角色，这样大家才能讨论出一个较为公正、合理的“游戏规则”。如果让事先已经在组织结构中占据某个角色的人参与讨论，不可避免的会有既得利益的影响，导致“游戏规则”不够公平，成为权利斗争的产物。 爱因斯坦一生奉献出的智慧本金带来的利息至他去世后仍源源不断，连他没讲过的话都归功于他 从本金、利息的角度阐述乱往名人身上塞“说过的名言”这一现象，有趣 风险投资的回报并不遵从正态分布，而是遵从幂次法则 《彼德·林奇的成功投资》中有类似的观点：投资一个10倍股可以抵消10多个看走眼的股带来的损失。统计学告诉我们世界是正态分布的， 社会科学告诉我们世界是2-8的（幂次）的。究竟两者占据的世界的划分板块是什么样的呢？ 顶尖法学院和商学院的新生欢迎词都暗含了同样的信息：“进入了精英学校，你的人生就高枕无忧了。”但这件事也许只有你不相信它，它才是真的 幽默地表述了一个真理。 希望每个刚刚高考完的孩子都能看懂这句话：） 将时间浪费在不能长久合作的人身上得不偿失。 如果你不能在工作上建立持久的关系，那么你就在浪费时间——即使纯粹从财务角度来看，也是如此 反思自己工作两年的经历，似乎浪费了很多时间、金钱。 要求把时间都用在以后回长久合作的人身上有点苛刻，但这句话至少得在之后的工作中起到一个警示的作用吧。 人们高估了科技与工程工作的难度，实在是因为这些领域的挑战显而易见。而销售人员在背后要付出很多才能使销售工作开起来很容易进行，而这些往往被技术精英忽视 自己也曾是对销售有偏见的一员。也许只有哪天决定挑战一下自己，去干一把销售才能真正理解这句话吧 最佳销售总是深藏不露。擅长销售的首席执行官没有什么不对，但是如果他的确看起来像销售员，那么他很可能拙于销售，更不擅长技术问题 做风险投资人可能需要这条建议","link":"/2017/01/07/bussiness/%E4%BB%8E0%E5%88%B01/"},{"title":"《重来》书摘","text":"豆瓣链接 当你推迟做决定，事情就会堆积起来，最后落得被遗忘的下场。只要有可能，就不要说“让我考虑一下”，而是“让我们做决定吧” 项目开发时间越长，成功的可能性越小（个人认为比较合适的周期是1周-1个月） 产品在精不在多（好的博物馆长会精心挑选展品而不是把所有的收藏都展示出来） 音乐就在你的指尖流淌（真正重要的是你的想法、技艺，不要过重的看待工具） 不要用抽象的方式来传递和沟通（抽象的事物，比如报告和文件，容易造成认知偏差） 当你需要和人合作的时候，采取被动交流工具，比如电子邮件，这样可以避免打岔降低你的工作效率（异步式的沟通） 会议有毒 会议要求人们做好充分准备，但大多数人根本没时间准备这个 会议中难免轮到某个低能人事发言，大家的时间都会被浪费在那个人的扯淡上 会议具有自我繁殖功能，一个会总能引出下一个会议 与会人员要精良精简 从明确的问题开始讨论 乘势而为可以事半功倍，没有势的推动，你哪儿也去不了。积累动力的方法就是完成一项任务，然后紧接着去完成下一项任务（不要把一件是拆分成很多碎片来解决） 不要逞英雄，对于任何耗时超过两周的工作，都让其他人来帮忙看一看（可能打开意想不到的方案） 人在困倦的时候，容易执着于眼前的错误方案，不愿意重新思考新的途径。于是终点成为海市蜃楼，而你最终深陷沙漠 熬夜想解决一件事的时候，这件事是否真的重要已经被抛在一边了 决策宜小不宜大（大决策很难一下做对，把大决策拆分成小决策的组合，让你能够立刻启动，也能方便转身） 不要简单的照搬（这样你不知其所以然，搬的知识表象，失去了对模仿对象深层本质的理解） less is more的竞争力（着重解决最重要的问题，把那些纠结的、困难的、令人厌恶的问题交给你的竞争对手去解决） 如果一味听顾客的，我就只能给他们弄一匹快马 — 亨利福特 拒绝顾客后，只要你花时间解释自己的想法，人们谅解的程度超过你的想象，甚至会赞同并追随你的思路 甘于低微（把事情做大、深入人心之后，不可避免的就要走稳健路线） 人不能不交流，同理，人也不能不去做营销工作 受不了时再招人（你往往发现自己需要的人远没有想象中的多，新的岗位都可以尝试先自己做） 求职信（沟通交流）能带来远大于简历的真实信息 企业文化不是自顶向下设计的，而是多年时光打磨出来的结果 想要做成一件事，就去找最忙碌的人（有人生规划、业余生活的人，而不是全部待在办公室耗时间的人） 如果有人犯了一个问题，直接和他讲就好了。为此设置一个规章制度是对个人过失的一种集体惩罚 不要轻易说ASAP，否则就是去了它本身的意义（区分事情的轻重缓急）","link":"/2021/01/09/acknowledge/%E9%87%8D%E6%9D%A5/"},{"title":"《精益数据分析》书摘","text":"豆瓣链接互联网流量增长日趋饱和，各大互联网公司对于流量的获取也逐渐的从跑马圈地转向精耕细作。大数据逐步从概念转向实际应用，如何将手中的数据“璞玉”雕琢成增长的玉如意也成为各个公司数据部门的头号目标。两者结合，在有限的流量下，用数据驱动，把每份流量的价值发挥到最大就成了新时代下增长的必经之路。 《数据驱动增长》一书由阿利斯泰尔·克罗尔写于2014年，如今已经过去四年多，其中的经验、框架依然不过时。针对美国互联网公司的一系列建议，放在中国，相当一部分也依然实用。下面摘录原书的一些观点，并附上自己的理解。 什么是好的指标 每个阶段有且只有一个第一关键指标。 不同的阶段你可以改变这个指标，但是要确保同一时刻只有一个第一关键指标。这可以避免让你迷失于过多的数据指标中。其实这点和机器学习也是相通的，不管什么算法，只能以最小化一个损失函数为目标，不是么？ 只有可以指导行动的指标才是好指标。 不能指导行动的指标没有意义。相对而言，一个比例指标更加能够指导行动。一个绝对值的指标往往是个“虚荣指标”（看着很美，对下一步决策没有帮助，甚至会误导决策）。 相关性与因果性。 相关性通常指在统计上呈现相关关系的两个随机变量（包括线性和非线性），是统计学上的概念。而因果性则是人类理解世界的一种逻辑关系。找到相关性很好，我们可以通过一个指标预见另一个指标的变化。找到因果性则更佳，你已经拥有了“改变未来”的力量（用改变因来改变果）。但是谨防把相关性当作因果性，这是一个很容易犯的错误，因果性有更为严苛的验证条件。 工具&amp;框架一些手段 市场细分：对目标人群做分层，不同层次采用不同的策略。 A/BTest： 很著名的手段了，但要记得先做好A/ATest(确保A/B两个分组在引入你的不同策略之前，在目标指标上表现是相当的)。 同期群分析：每个用户都有其生命周期，把处于不同生命周期的用户区分开来分析能够洞见一些更深层次的问题。 一些陷阱 数据呕吐：有太多指标，互相干扰，反而无法给出一个明确的行动指导 假设数据没有错误、噪声：用尽各种高级手段没法提高的时候，回过头来看看，你在数据处理的时候是不是有一些不明显、却很致命的“小错误” 简单排除掉异常点 ：那些看起来很诡异的异常数据，不要简单扔掉它，深入追究下去，不是给你带来一个全新的idea就是给你指明一个隐藏的bug 分析的时候包含异常点：异常点不应该简单扔掉，同样也不应该带入到数据分析中去。一个支点可以撬动地球，一个马拉松中跑偏的第二名可以带偏后面的所有人，一个异常点也有可能将分析结论导向完全错误的方向。。。 精益创业画布 创业之前，先考虑下面9个问题： 1. 问题：当前有哪些需要解决的问题，当前的解决方案是什么样的 2. 客户：需要解决的问题的潜在客户是什么样的，有多大规模 3. 解决方案：针对每个问题，一一列出解决方案 4. 独特点：一句话阐述产品的独创新点 5. 渠道：如何触达客户，获客成本 6. 营收：如何赚钱 7. 成本：运营、开发的成本 8. 护城河 你的技术、市场壁垒 9. 关键指标用来指导精益创业 6种商业模式 电子商务(京东、亚马逊)： 用复购率区分网站运营的重心，是拉新还是复购 Sass（salesforce、有赞）： - 构建合理的分级收费模式很关键。 - 流失率等于一切。 - 需要客户成功团队。 移动端应用（微信、手游）： 考虑付费下载 or 应用内收费方式。 app更新的方式限制了A/Btest的应用。 需要在免费玩家和人民币玩家之间做平衡 媒体网站（新浪、搜狐）:广告收入 用户生成内容 UGC（微博、facebook）：web2.0的代表，初期面临冷启动的问题（由内容才能吸引用户，有用户才有内容）。访客参与度意味着一切 双边市场（淘宝、租房网站）：重点需要关注被需要的一方（有钱的一方）。有钱赚，另一方自然就活跃了。市场的商品的丰富度至关重要 创业的5个阶段 移情 和客户访谈，获取他们真实的想法、痛点 从定性到定量，分析解决的问题的市场范围 粘性 确保用户在你的产品上的活跃度。 确保用户不会轻易流失之前，别急着导流量 不要让注册账号等细节阻碍核心功能的实现 开发功能前，问以下问题： a.这个功能有什么帮助（对留存率） b.这个功能的时间成本 c.效果是否可以衡量 d.是否会让功能过于复杂 e.用户怎么说（怎么做更重要） f.每周列出下周做的功能点，并写出衡量的指标、证据、目标 病毒性 病毒系数: 平均发出邀请数*接受率 病毒系数&gt;1即可自行不断增长 病毒传播时间也至关重要 营收 确保客户终身价值大于获客成本 规模化 中等规模容易找不到自己的“利基” 形成纪律 关注下一个市场 AARRR(海盗指标） Aquision 用户获取 Activate 用户激活 Retention 用户留存 Revenue 营收 Refer 用户自传播 底线在哪里（一些指标衡量的建议）关注的一些指标什么样的值才算好 平均数远远不够好 每周活跃用户增长5% 30%的注册用户每周访问一次， 10%的用户每天都访问 获取客户的成本不要超过能从他身上获得价值的1/3 病毒系数：0.75 优化一个指标成效越来越小，就该去换一个指标了 后记《精益数据分析》是一本同时适合产品、运营、技术看的书。尤其是技术，因为只有技术才能不依赖于取他人将精益增长的想法用实验的方式实现出来。精益增长是大势所趋，愿你不止做一个将需求文档翻译成代码的技术💪🏻","link":"/2019/02/21/bussiness/%E7%B2%BE%E7%9B%8A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"title":"《重来2》书摘","text":"豆瓣链接 未来已经在那儿了，只不过若隐若现而已 如果你问别人，必须把工作做完会去哪儿，很少有人会说办公室 办公室有太多的干扰 工作属于创意性工作时，固定的日程表有害无利 编程在某些方面也属于创意性工作 想想看，你给仅仅隔着三张桌子的同事发邮件了多少次？ 即使在办公室创造了近距离沟通的机会，很多时候我们也倾向使用远程沟通的手段 如果我不盯着，怎么知道员工在工作？ 人满足最低期望的能力高极了。你认为别人是懒汉，他们就会证明你是对的。不断试探、最终表现在你的最低期望左右 在家干扰太多了 干扰的头号劲敌是有趣又有意义的工作，而不是环境 如果没法保持专注做你的工作，可能错不在你，而在工作的内容并没有意义或者无趣 不是每个问题都立刻需要得到解答 没有什么比为不需要立刻得到解答的问题去打扰别人更加傲慢的事儿了 把工作成果show出来。糊弄同事可比糊弄老板难多了 想看一个人有没有出成果，让他把工作内容想大家展示出来，而不是汇报给老板。同事对这件事的工作量判断要精确得多 远程工作之后，评判一个人成果的标准就剩下一个：他今天到底作了什么活儿。把是否准时到办公室、是否和同事相处融洽这些无关的因素都排除掉了 M&amp;Ms(Meeting and Managers会议和纯粹的管理者。需要慎用) 会议应该像盐，小心翼翼的像调味品一样撒上一点，而不是一勺一勺地往上倒 工作就像一场马拉松，找到适合自己的节奏，持之以恒地发展才是长远之计 参与远程项目时，要让客户也参与进来，让他们感觉到这也是他们的项目。当自己也成为项目的一份子之后，焦虑和恐惧会被兴奋和期待代替 远程工作可以加速优胜劣汰的速度 尝试做一些你员工的工作。如果你不知道他们工作的复杂程度，管理将会变得很艰难 多做one on one沟通。一些简单的沟通能够避免负面情绪积累 移除路障 不要为了避免犯错儿设置过多的限制。人都会犯错，只要不是有意的，都会变成成长的经验 远程工作需要注意的不是避免偷懒，而是避免过劳 随时都能接收到工作的消息，会忍不住想回复或者想解决 面谈是稀缺资源，要妥善使用 在办公室上班，随时能当面谈，这就是去了其稀缺性。容易变成效率低的扯淡 通过着装或者一些小暗示，让你区分出工作和生活很重要 在办公室上班，喜欢强调工作和生活是一体的远程办公时，生活和工作天然融在一起，然而需要更多的注意区分出两者 30年后，技术进步了。人们回顾过往，会惊讶的发现，怎么会有办公室这种奇怪的东西 变革的四个阶段：起初他们忽视你，进而他们嘲笑你，然后他们反对你，最后，你赢了！","link":"/2021/01/20/acknowledge/%E9%87%8D%E6%9D%A52/"},{"title":"《激荡三十年·上》书摘","text":"豆瓣链接 《激荡三十年》分上下两本，本文摘抄下册的一些让人有感而发的句子以及自己的一些启发、随感。 1993年5月10日，北京市政府正式宣布，从这一天起，取消粮票。 粮票作为一个年代感十足的东西，常常被老一辈提起。 没想到我出生的头三年里尽然和它也有交集。规则是死的，市场是活的，每个政策都有一个对应的市场价。 1994年前后，中国商业的火山口就在保健品和饮料食品领域。 每个时代都有其火山口，饮料、保健品、电器、手机、互联网、下一个火山会是什么呢？ 一直到2008年，中国成为全球最大的家电制造基地， 但是仍然无法完整地制造出一台百分百的“中国彩电”、“中国冰箱”或“中国空调” 到如今这个状况已经被打破了。百分百的“中国制造”也许只是名族情怀的需要。在商业逻辑里，先占领市场再追上技术也许是更加快速、正确的一条路呢。 在广告营销上，赵新先有两个发明。他是出租车顶箱广告的中国发明者，也是明星广告的首倡者。 他是在1988偶尔瞥见一部美国电影的车顶广告箱后想到的这个创意。看电影也能发掘一个巨大的商机，有点神奇。足见当时中美在商业上的巨大差距，现在也许过了照抄美国模式的好时代，照抄科幻电影呢？ 1993年， 无锡红豆集团搞产权改革的时候比较保守，私有股份没有超过50%。当时这个企业在无锡排34位，而14年后，前面的33名都不见了。 商业的竞争残酷，夹杂上政治就是残酷加上莫测。也许跟物竞天择的逻辑类似，最终留存下来的不是最强的，而是最谨小慎微的。 在传统民营企业为了产权转移而各种曲线操作甚至搏命的时候，互联网企业发展迅猛， 它们都有着清晰的产权结构，这是一个没有“制度原罪”的产业。 来的早不如来的巧，后发优势。 如果要为这部30年的企业史选一个标本人物——只选一个，史玉柱可能是最典型的。他的经历似乎在证明：成功是一种了不起的除臭剂，它可以去除你过去的所有气味。 历史总是任成功者打扮。即使社交媒体越来越发达，强人已经没有办法控制媒体。但是他们身上的“成功”光环会让人们选择性的看轻那些不太光彩的事实。终究还是要以成败论英雄呀！ 健力宝——一家蒸蒸日上的企业，因为政府与经营者的产权博弈而变得前途无比莫测。 双输的一个案例。非常遗憾，而今还常常能在硝烟中想起健力宝汽水的味道，和那些无所事事一整个夏天的年少。 在某种意义上，这些海外投资人成为分享中国改革成果的最大获利者之一。 金融的手段，让我们不仅可以分享本国发展的果实，还可以分享别国发展的成果。如同别人分享中国的发展，再中国发展速度放缓的今天，我们的目光是否可以放倒印度、越南、朝鲜？ 所有的成功都是抵抗诱惑的结果。财散人聚，财聚人散。垄断没有好跟坏， 只有坏或更坏。 非常认同 冰棒理论：国有资产就像太阳下的冰棒，如果不把它“吃掉”，那么它也会完全融化掉、浪费掉。 一些人收购国有企业发了大财，创造出这么个理论。看着还有些道理，收购国有资产之后，自然是这些人占据了属于人民的财富。然而，如果没有收购，人民的财富都是怎样融化掉了呢？国有企业的低效率浪费掉了工人应有的能量？ 超乎想象的财富是任何人都难以适应的。财富是违反自然的，有钱人的行为往往表现出彻底的适应不良。 在为事业拼搏为财富奋斗的同时，不能忘了提高个人的审美、知识、思想深度。要两条腿走路，不然会被财富的大长腿带的行动畸形。当然，就现阶段，财富的腿长得更快些也无妨，哈哈哈！ 中国加入WTO之后的“反倾销第一案”是欧盟诉讼温州打火机企业的。最终由于“中国企业没有做亏本买卖”，欧盟撤诉。 是否在低于成本价格销售是判断是否倾销的关键。个人仍然认为“反倾销”是横加在市场规律上的不合理的人为规则， 或许低于成本销售也只是一种商业策略呢？近年屡见不鲜的互联网企业烧钱赔本赢得客户就是对反倾销法的一大反击。 贾樟柯拍摄《三峡好人》时发现5个月的电影拍摄竟赶不上场景的变化，即使摄影机镜头保持静止，5个月后，里面的空间早已面目全非。 所以要游览祖国的大好河山要趁早，可能过个几年，就算去了西藏、去了敦煌、去了洛阳、去了凤凰，也找不到它们在大师笔下的味道了。记得90年代小时候去过的景点和现在的就味道大不一样。","link":"/2017/05/13/history/%E6%BF%80%E8%8D%A1%E4%B8%89%E5%8D%81%E5%B9%B4%E4%B8%8B/"},{"title":"《国史大纲·下》书摘","text":"豆瓣链接 国史大纲这本书2012年就买了，2016年才读完。还是囫囵吞枣的读完了一遍，实在惭愧惭愧。自己从小喜欢历史，但那时候无非是对一些王侯将相的故事感兴趣。喜欢一些战争、权谋、兴替的情节。至于政治体制、文化风情、民族兴衰，对于年幼的我还太深奥，也不那么有趣。小时候读的是《中国通史》一套书，总共六本，从先秦到明清，类似连环画的形式，很过瘾。而《国史大纲》这本书，显然又是更高一层面的通史书了。开始读的时候，有些吃力，竖版排版的繁体字，对于一个长久生活在大陆的人，还是有点陌生。书中有很多字体很小的考据，有些过于翔实，读起来容易让人忘记了主题。后来我就挑选字体较大的正文阅读，小字体的捡感兴趣的扫一扫，这才赶上了进度，吞完了这本书。好书一定是要读第二遍的，或许一年后，或许十年后。总之，等自己阅历增长了，一定能读出另一番味道的。这本书到了下册的南北文化之转移一章，我才开始了系统的读书笔记，记录一些自己觉得精辟的评论，一些总结。就暂且从这里开始记书摘吧。 南北经济文化之转移 唐安史之乱后，中国经济文化开始向南转移。无论从粮食、经济、文化（进士人数）、州县划分上。南方逐渐远远超过北方。人口比例直至10:1。 纠其原因：或疑黄河为中国之患，长江为中国之利，这种历史言论，站不住脚。实则黄河的水患起落，与北方的兴衰呈平行状态，互为因果。 黄河患因： 常常为了其他原因，改了正道（多为政治斗争） 政治腐败，河工黑暗（亦有运河之害。 豫鲁苏皖四省，耗大量精力在河、淮、运三水的防范上） 北方外族的破坏：一见于唐代后期的藩镇割据二见于五代兵争（此两期间的政治黑暗，达到极点）三见于辽宋对峙四见于宋夏对峙五见于金人统治六见于蒙古残杀（蒙古对金的残害胜于宋，而金汉人南逃，也对人口变迁有重大影响）七见于元之黑暗政治八见于元末北方残破 盖北方之状态，先坏于唐安史之后，大毁于宋之南渡，至蒙古灭金而摧残益甚。 不过，钱穆先生认为北方之重新复兴必为中国之复兴重要一部分 江南水利的兴修 江南本不适于农耕，历代政府着重对其开发，以致江南米粮占天下很大一个比例。魏晋南北朝时，江南米粮还要依靠荆襄接济。 太湖地低，沿海地高。 古环湖常有水灾而沿海多旱 政府对于水利的兴修，南宋一百五十多年，仅一两次水灾（竟甚于今日！！！） 南方的农业兴起，带来了兼并之事。大家有奴使多至万家者。 明代，苏、松、常、嘉、湖五府（太湖圈），据天下八成赋税。（今日，产业结构已经截然不同，然分布未变，余猜测人才分布乃本因尔。） 逐渐至明代，对于江南赋税日重。唐中叶至明代以来，北方的财富已经全部转移到南方来。明代南方民众的生活，却远不如唐中叶以前北方的民众。此乃赋税之因。这也是明代国运不如唐代的一个绝好说明。 社会自由讲学（类比先秦）之再兴起 贵族门第逐渐衰落 科举制兴起 政治权利更加普遍 社会阶层更加消融 雕板印刷最初始于唐代。唐咸通九年（末期）的敦煌石刻金刚经是最早的雕板书籍。 至宋又有活版印刷之发明。 庐山白鹿洞书院、嵩阳书院、岳麓（lu）书院，应天府（河南商丘）书院 是古代四大书院。 宋明社会，需要一种力量，上监督政府，下援助民众。宋明学术界由此兴起。程朱理学，一直是被批判的一种思想。作者在书中似乎却有另一种认识，认为这种思想对于当时的社会有很大的帮助。 程朱理学认为：公是天利，私是人欲。公是义，私是利。 学校之教日衰，讲学之风日盛。（为何却未出现类似今日的“科举辅导班”讲学？有趣） 开发民智，教育人才为第一步，改进政治为第二步，创造理想国为第三步。 在我谓之性，在外谓之理。 《大学》一书 先秦曾子注：明明德，新民，之于至善为三纲领 格物 致知 诚心 正意 修身 齐家 治国 平天下 他们的思想类似于自负天下为己任的秀才们中间的宗教——秀才教（有趣） 清代之部 崇祯皇帝徒知责下，不知反躬 法在渐，不在骤 元明清三代无藩镇割据之忧，而不能禁乱民之平地突起以为祸（元末农民起义，明亡于流寇，清有太平天国治乱，甚至武昌起义） 三藩不自安，于康熙十二年请撤藩，竟得许（一个“竟”字，有点冷幽默） 吴三桂身为汉奸，不受国人信仰，乃起事不成之一因 努尔哈赤极端排汗，而皇太极则改用怀柔政策（心不一样大了） 明大臣议事，皆称卿，清代直呼尔，可见皇权渐增，臣位渐卑 清用汉人，先旁省，而后江浙 地方扣留财富，不解中央，其事始于咸丰年间，正是各省满人淘汰，汉人重新启用之时。 清廷以山海关外为东三省，其政治制度与关内不通，作为退一步的退路。这种闭塞之举，也导致了近代东三省的文化落后。 康熙五十年所谓盛世人丁者，尚不及明万历时之半数。（现代化之前，战乱对人口经济影响巨大，常常容易出现后代不如前代，如今则难出现） 清廷能操纵民间学风，得益于其对汉族文化的深入了解。 洪、杨的耶教传播，激起了传统读书人的反对，他们的骚乱政策，惹起了一辈安居乐业的农民的敌意（失败的原因之一） 曾（国藩），左（宗棠），胡（林翼），李（鸿章）：同治四功臣 清末改革，主张：速变，全变，而无一个按部就班的切实推行之条理","link":"/2017/01/07/history/%E5%9B%BD%E5%8F%B2%E5%A4%A7%E7%BA%B2%E4%B8%8B/"},{"title":"《DKN: Deep Knowledge-Aware Network for News Recommendation》","text":"论文链接 目标：将知识图谱应用到新闻推荐中 input：用户的历史点击的新闻title+候选新闻title+通用知识图谱 output：ranking of candidate news 总体架构： Knowledge distillation 用实体链接技术，将文本中出现的实体，链接到KG中的entity knowledge graph Embedding：translation-based knowledge graph embedding methods （Knowledge Graph Embedding via Dynamic Mapping Matrix） KCNN（knowledge-aware CNN） 用linear mapping方法，将wordEmbedding, entityEmbedding, entityContextEmbedding 映射到相同的维度，作为输入句子的三个channel 用TextCNN将输入句子encode成一个vec Attention-based User Interest Extraction user点击过的所有text的embedding列表作为user的特征 用候选text的embedding对user点击过的text的embedding列表做attention得到最后的特征向量 最后接一个sigmoid，做点击率预估 疑问： 依赖用户的历史点击新闻，不能解决冷启动的问题？ graph embedding是基于全图做，还是基于text中提及的entity构成的图做？ 如何基于通用KG，做基于entity的预训练？ 启发： 如果用CNN的方式，不同渠道来的特征，可以对齐之后当做不同channel的数据来用 Attention的妙用，在recommend领域（只要是衡量item和list的关系，都可以用）","link":"/2019/10/30/nlp/DKN-news-recommend/"},{"title":"《GPT-based Generation for Classical Chinese Poetry》","text":"论文链接 目标：根据输入的格式（对联、绝句、律诗、词牌名） + 主体（诗名、词名、藏头诗的头），生成相应格式的对联、诗、词 诗歌生成的难点： 生成文本需要满足相应的诗歌类型的格式（长度、对偶、押韵、平仄等） 生成的文本需要主题一致，如果给定主题的话，需要和给定主题一样 之前的做法： 用基于constraint 或者基于template的方式满足格式 用插入关键词的方式满足主题一致 需要引入比较多的人工规则和特征 本文的做法： 基于GPT的生成模型 在中文新闻的预料上预训练 在诗词预料上finetune。 输入格式为 藏头诗用藏头诗的头代替主题词， 对联用上联替代主体词 在诗歌预料训练时间比较短，如果overfit，倾向于输出原句 用topK sampling的方式保证生成结果的diversity（每个字的输出对topK的词做sampling，选择一个） 结论： 出人意料的，只用一个简单的GPT模型，输出的结果基本都能满足格式要求（训练预料都是严格满足格式要求的，而GPT能捕捉这种规律） 在给定词牌名生成词的情况下，满足格式的表现差一些（词牌名对应的样本比较少，远远少于绝句、律诗） 启发： GPT是一个很强大的生成式模型，而BERT一般只用来做特征提取器，两者区别还是比较大的。 后续要仔细再读一下GPT 以及 GPT2的论文 可以把诗人的名字也加入到输入中，这样可以看看同样的主题、同样的格式，不同的诗人会生成怎样风格不同的诗？ 对于BERT/GPT这些基于语言模型的预训练模型来说，输入直接用自然语言的形式比较好。 比如（“律诗”，“绝句”）直接是自然语言拼接到输入中，而不是用两个特征embedding concat到输入上","link":"/2019/10/31/nlp/GPT-Chinese-poetry/"},{"title":"《激荡三十年·上》书摘","text":"豆瓣链接《激荡三十年》上下两本是今年3月份就读完的书了。一直放着没有写书摘，一方面自己懒着了，总觉得写博客是一件非常”隆重”的事儿，不愿意开始。另一方面，全书侃侃而谈700多页，依然只是对这历史上浓墨重彩的三十年的一个轮廓的草草勾勒，担心自己只是流水账般的复述这些历史事实而缺少自己的思考。其实，把内容摘下来并附上一些自己简短的感想何尝不是一种思考呢？ 这些更加接近于”原始数据”的书摘正是进一步思考，或者和志同道合的朋友们交流的基础。 一个人要让自己快乐其实是一件不难的事，你只要给自己一个较长时间的目标，然后按部就班地接近它，实现它。结果如何，在某种意义上可能是不重要的，重要的是，在这个过程中，你会非常的单纯和满足。 做一些需要不断积累才能完成的事儿， 这个过程中的幸福比一蹴而就的惊喜更加长久。 比如健身， 比如日记， 比如维护一个开源项目。 国企改革的核心是产权制度改革。 所有权甚至是一切制度的根本，拥有所有权的人才有做的更好的动机。 刘桂仙，获得全国第一张个体餐馆执照。 开在北京东城区翠花胡同， 曾经副总理登门给她拜年。 如今（到2008年）这家餐馆依旧在那条日渐衰落的胡同中。 1983年前后，中国第一批’倒爷’出现在北京和深圳。 前者是政策资源和权钱交易的中心，后者则有一个宽松的商业氛围和对外开放的窗口效应。 如今互联网时代让传统的’倒爷’已经没有了生存空间， 然而互联网时代，真的就没有东西（信息）可以”倒”么？ 王安是第一个全球意义上的华人企业家。 美国的1.5代华人，同时受中西方教育影响，最终传统的家族式企业观念还是阻碍了王安电脑的发展。 戴尔和思科的出现，基本代表了新技术公司的两条成长之路：独一无二的商业渠道模式，或者高度垄断的核心技术优势。 中国在全球范围内有这两样之一的新技术公司有哪些？ 1985年初， 对很多重要生产资料施行双轨制：体制内一个价格，体制外有个远远高出这个价格的价格。这给了”倒爷”巨大的利润空间，实施双轨制带来的损失每年差不多占GDP的9%，在不少年份和GDP增长持平。 有横加于市场之上的各种规则，就有利用这些规则大发横财的钻营者，以及靠给规则开后门的政府腐败官员。 和GDP增速差不多的价格差导致的损失，流入了个人腰包。 相当于全国人民为那么少数的一个群体努力了一年， 何其可怕！ 党委书记在企业中的指责——–”大事不糊涂，小事不纠缠”。 很精髓，理想中的状态。 1985年， 一个叫格里希的德国人成为了第一位外籍国营企业厂长。 管理权与所有权分离的思想的表现，他的铜像在武汉正阳街工业区广场中央（可以去看看）。 1985年的温州”抬会” 一度非常疯狂，一个庞氏骗局， 席卷整个温州，甚至出现投资者把钱往会主家扔的情况。 思维活络的温州人，让全国人民第一次感受到了资本强大的力量。 1999年，华西村股票上市，这是中国第一个在资本市场上融资上市的村庄。 村庄也能上市，让人脑洞打开。 也是那个年代村庄里一个强人独揽政治、经济大权，把村庄当作企业管理，带领全村百姓致富的独特景观。 1987年， 肯德基在中国的第一家餐厅在前门繁华地带开业， 并且任命了一个中国出生的职员出任中国公司总经理，在餐饮文化上充分和中国融合。 比麦当劳早了3年，后者花了近20年也没有追上。 如今似乎肯德基渐渐式微了啊， 先发优势挥霍殆尽了么？ 价格管制是天下最事与愿违的政策。 追逐利益是永动的本能。 中国银行成为第一个在世界500强中的公司。 国家对国企（亲儿子）以及民企（私生子）的巨大差别待遇，反而催生了民企在逆境中顽强生长的能力， 有些小国企反而越扶越不上墙。为此国家决定放弃众多小亲儿子，培养及格的儿子，中国银行就是其中之一。 1990年，第十一届亚运会在北京举行， 为沉闷两年多的企业界带来的很多商机。年轻的长城、联想自主开发了运动会所需的软件系统，90%采用了国产货，不过为了防止意外，组委会还是请来了IBM做信息备份工作。 亚运会这样重要的盛会， 国有企业在计算机领域没有涉足，处于对民企的不信任，最终请来了外企。 中国改革的一个特点便是，人民的实践有时候会走在中央政策的前面。 人民有很强的创新能力，特别是在政策的边缘地带起舞。 中央政策在静观其变之后再沉稳的跟上，其实是一种很好的策略。只是，那种率先游走在灰色地带的英雄的命运，很大程度上就要看政府的脸色与运气了，向这些智者+勇者致敬！ 欧美对中国展开反倾销的同时，跨国公司在中国的倾销性活动加快了步伐。 反倾销本就是一个很奇怪的强加在市场作用上的规则。只为保护既得利益者，让发达国家的工人们可以继续慵懒的过活。 1992年，中国经过15年的变革之后， 一只脚踏进了互联网的河流中，并不比发达国家晚， 在这个意义上，中国是幸运的。 互联网革命是重大的一轮革新，中国幸运地在互联网兴起的时候做好了准备，搭上了这班车。今天看来，中国的互联网走在世界的前列，除了垄断国企，中国市值在世界前列的公司无一例外都是互联网企业。 这是一条可以实现超车的近道：美国没有经历过工业革命，却直接在电气革命这轮浪潮中超越了英国。 《物种起源》中有段对于”丛林法则”的经典论述：”存活下来的物种，不是哪些最强壮的物种，也不是那些智力最高的物种，而是那些对变化作出最积极反应的物种”。 在企业竞争中依然非常适用。","link":"/2017/05/13/history/%E6%BF%80%E8%8D%A1%E4%B8%89%E5%8D%81%E5%B9%B4%E4%B8%8A/"},{"title":"《Improving Language Understanding by Generative Pre-Training》","text":"论文链接 目标：用与训练的LM模型提升NLU任务的效果 基于大量未标注语料训练的两个问题 如何设置合理的训练目标？LM/NMT/discourse coherence? 这也是GPT和bert的区别之一 如何将预训练的模型得到的表征应用到下游任务中去？ 模型结构 一个没有encoder-attention的transformer decoder 给定一个窗口的输入(最后n个token)，预测下一个单词 模型返回的是最后一个token对应的embedding，而不是真个窗口sequence的embedding fine-tune 将结构化问题的输入变成一个sequence。不同的field的输入在sequence里面用delimiter区分开来 句子的最后加上[Extract] token。 用来训练成为整个句子的embedding。 这点和bert的[CLS]类似 decoder是只能attention到前面的token的，所以输入句子的顺序很有影响（对比bert，用的是LML，不收影响）因此做两个句子的相似度检测，需要交换两个句子的顺序，分别输入模型 可以把LM和目标任务联合训练。 将两者的loss加权求和 实验 随着transformer的层数增加，模型fine-tune之后的效果越来越好，每层都能捕捉到新的信息 做zero-shot预测，随着pre-train的step变多，效果越来越好。说明预训练是能捕捉到一些通用的语义的东西的 fine-tune的时候加上LM损失在大数据集有帮助，小数据集上表现不如不加 GPT 和BERT区别 GPT是LM预训练，BERT是MLM+NSP预训练 GPT的self attention只能attention到前面的token， bert是双向的 GPT做下游任务fine-tune的时候，多个输入的顺序是有影响的。比如text entailment任务，premise要在hypothiesis之前,bert没这要求（原因见上一条） GPT可以直接做生成任务，而BERT不能做生成任务","link":"/2019/11/07/nlp/GPT/"},{"title":"《KG-BERT: BERT for Knowledge Graph Completion》","text":"论文链接 目标：做KG-completion 思路：基于预训练的BERT，做SPO三元组的embedding， 不依赖原句 训练 将SPO三元组拼接成[CLS]S[SEP]P[SEP]O[SEP]的形式， 其中S和O用同样的segment embedding S和O是entity的name或者description 用二分类判断三元组是否正确， 或者用多分类，给定S\\O 判断relation的类型 预测 给定SPO，判断是否正确 给定SO，判断relation在schema中的哪一个 给定 SP，判断O是哪个（将所有可能的O列举，拼接成三元组之后预测），按照得分排序取第一个 启发： 不依赖上下文，只依赖三元组的文字表达，利用BERT的大量预训练，做分类任务。听起来不太靠谱但是可以尝试 对于训练预料的采集要求降低，只需要知道三元组的列表，不需要关联到相应的文本中去","link":"/2019/11/19/nlp/KG-BERT/"},{"title":"《Matching the Blanks: Distributional Similarity for Relation Learning》","text":"论文链接 目标：基于大量未标注语料，训练一个relation表征的模型 input：relation statement(x, s1, s2) output：relation representation: 一个稠密向量，使得两个关系越接近，两个关系的表征向量点积值越大 bert-based architecture: 预训练 There is high degree for redundancy in web text, relation between tow entity is likely to be stated multiple times 两个不同的句子中，如果包含相同的实体对，这个实体对在两句话中大概率表示相同的relation 两个不同的句子中，如果包含不同实体对，这两个实体对大概率表示不同的relation 例子: 结构: 实验细节 采用维基百科数据，用google cloud natural language api做实体链接 以α = 0.7的概率将entity mention 替换成[BLANK] negative sampling with relations share one entity 生成了600M的实体对,正负样本1:1 实验结果 在supervised training任务下，有细微提高，可以提高模型收敛速度 在Few Shot任务下，提高明显: 启发 想办法在未标注数据中提取有用信息，在大规模数据上做预训练，然后在标注任务中做fine-tune是提升任务效果的比较好的方案（BERT,GPT…）。这种方案相比在给定的标注数据下，优化模型的结构，往往提升更多 我们爬取了百度百科的数据，可以利用百科网页的链接+简单的实体链接技术，在百度百科的数据上预训练一个中文关系表征模型 可以基于关系表征模型做聚类，用来发现新的关系，进而做一个schema-free的抽取系统","link":"/2019/08/02/nlp/MTB/"},{"title":"《Text Generation from Knowledge Graphs with Graph Transformers》","text":"论文链接 解决问题：给定一篇论文的title +通过知识抽取工具从论文abstract里抽取出的知识图谱，用生成式模型生成文章的abstract 整体框架：用BiLSTM encode title, 用Graph Transformer encode 知识图谱。 decode过程中同时可以attention到title和图谱的encode特征，同时加上copy机制 细节：graph transformer用节点相邻的节点作为该节点的context，其他和text transformer类似 细节：原始图谱的边是有标签且无向的，在做graph transformer之前，将原始图的边改造成两个节点，这样得到的图的边是有向、无标签的。 同时加上一个global的节点，和所有节点都连接，让整个图联通 数据集：自建数据集，包含40k论文 评价指标：人工打分+BLUE+METEOR 结论：引入知识图谱的模型明显好于只用title的模型，graph transformer也好于Graph Convolution以及Graph Attention的模型 问题：生成的abstract对于图谱中的entity的覆盖率只有60%, 需要更好的机制保证entity的提及 git: https://github.com/rikdz/GraphWriter","link":"/2020/03/10/nlp/TextFromKG/"},{"title":"《Beyond Accuracy: Behavioral Testing of NLP Models with CheckList》","text":"论文链接 这篇是ACL2020的最佳论文。论文指出现有的模型效果评估方案的问题，同时借鉴软件测试的方法，提出了一种全新的NLP模型测试方法（个人认为迁移到CV领域也不麻烦）CheckList。这种测试方案可以帮助人们更清晰、系统地了模型各个方面的优缺点。 目标问题对于NLP任务，传统的测评方案是在测试集中计算Metrics(Accuracy, F1…)， 这种测试方法有如下缺陷： 测试集可能和训练集有同样的Bias, 在测试集上的指标不能准确衡量模型在实际应用时的效果 用一个或几个数字衡量模型的效果，不利于发现模型到底在哪个方面有缺陷，也难以指导模型调优 存在高估模型能力的可能 GLUE榜单上，有些SOTA模型的指标已经接近甚至超越人类了。真的没有提高的空间了么？ 模型会走捷径，只是在测试集中能答出正确答案，但是没有真正理解任务。 虽然模型回答正确了测试集中的问题”what is the moustache mad of?”。再深入问几个问题，发现模型并没有理解问题，只是狡猾地走了捷径——“只要问有关What的问题，就返回图片中最显眼的一个物件给你” 模型抗扰动性差 问题里多加一个问号，模型就不会数数了 模型没有逻辑一致性 模型只是学习概率分布，没有背后的逻辑，出现前后矛盾的回答。 解决方案借鉴软件工程中的黑盒测试思想，用不同的测试方法测试模型的不同方面的性能。将结果输出成一个表格形式的checklist。 测试目标测试目标是与目标模型无关的，NLP模型都应该具备的一些基础能力，包括： Vocabulary+POS: 理解重要词汇以及词性 Named entites: 识别出命名实体 Nagation: 识别出否定词 Taxonomy: 理解同义词、反义词 Robustness: 对于typos, 无关紧要的改动的抗干扰性 Coreference: 理解指代 Fairness: 不具有性别、种族歧视 Semantic Role Labeling: 理解语义角色 Logic: 理解语义中的对称性、一致性等 Temproal: 理解时态 测试方法对于不同的测试目标，有不同的与之适配的测试方法，本文提出3中测试方法，包括： MFT(Minimum Functionality Test) 最小功能测试，类似软件工程中的单元测试 针对某个测试目标，设计一个最简单的针对那个目标的测试用例 需要测试用例设计者知道正确的label 例：对情感分类任务测试Vocabulary+POS能力。This is as great flight -&gt; positive (测试识别”great”) INV(INVariance Test) 不变性测试, 验证模型对于变化后的测试用例是否输出结果不变 测试人员不需要提前知道正确的label 需要模型前后两次输出结果的label不变且probability 变化不超过0.1 例：对情感分类任务测试Named entites能力 输入1：AmericanAir thank you we got on different flight to Chicago 输入2：AmericanAir thank you we got on different flight to Dallas 前后两次输入只变化了城市实体Chicago -&gt; Dallas。不应该对情感分类模型的结果产生变化 DIR(DiRectional Expetcation Test) 定向期望测试，期待对样本变化后，模型输出结果的probability往某个方向变化 测试人员不需要提前知道正确的label 例：对情感分类任务，测试Nagation能力 输入1：JetBlue, why won’t you help me?! Ugh 输入2：JetBlue, why won’t you help me?! Ugh. I dread you. 期待模型能识别出否定词dread, 进而模型输出negative的probability增大 测试构建 对于待测模型，构建一个由测试目标和测试方法组合而成的checklist 对于每一项具体的测试，构建测试用例，评测模型在该项任务上的failure rate 对于MFT测试集，可以通过简单的模板构建 可以使用预训练的Masked Language Model提示候选词，帮助快速构建测试集 使用WordNet等外部知识，做同义词、反义词替换 作者开源了测试集构建项目：https://github.com/marcotcr/checklist 实验结果 在情感分类和重复问题检测两项任务上，对商业模型(微软、谷歌、亚马逊)以及学术界SOTA模型（Bert、Roberta）做check list测试 在很多测试任务上，这些模型的failure rate都很高，这是传统的测试集Metrics没能发现的问题 一些值得注意的测试结果 对于情感在时态上的变化识别不好: I used to hage this airline, although now I like it -&gt; pos 对于句子结尾的否定识别不好: I thought the plane would be awful, but it wasn’t. -&gt; pos/neutral 作者的情感比其他人的重要：Some people think you are ecellent, but I think you are nasty. -&gt; neg 一些语料中的偏见（男性是医生）：John is not a doctor, Mary is. Who is a doctor? -&gt; Mary check list对于那些商业用途的，非纯模型的api一样有效 利用测试集构建工具，非领域专家人员也能快速构建checklist的测试集。比传统的测试方法更加高效 总结与启发 测试集上的指标可能会让人高估模型的效果（模型走了捷径）。上线之后对于模型预测结果的抽检是非常有必要的质量监控手段 checklist的测试可以从不同维度评估模型的能力。从而发现模型在某个NLP通用能力上的缺失。但是如何修复这些问题呢？模型对于我们来说是黑盒的，不能像软件工程测试出程序bug之后改代码来修复。或许只能添加规则，做pipeline来解决这些问题。 checklist证明了模型学习的时候是急功近利的，尽可能找到一些训练集/测试集上的捷径来降低loss/提高指标。将checklist中列出的NLP模型通用的基础能力(Taxonomy/Temproal…)做成与训练任务，或者加入到目标任务的联合训练中去，是否能提高模型在checklist评估中的效果？ 相关链接 https://zhuanlan.zhihu.com/p/182557001 https://medium.com/@caiweiwei1005/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-beyond-accuracy-behavioral-testing-of-nlp-models-with-checklist-690487be3135","link":"/2021/04/15/nlp/beyond_the_accuracy/"},{"title":"《Vocabulary Learning via Optimal Transport for Neural Machine Translation》","text":"论文链接 解决的问题在机器翻译的任务中，合理选择词表和词表的大小至关重要。论文基于Marginal Utility（边际效益）这一经济学概念，提出通过最大化的Marginal Utiltiy of Vocabularization（下文简称MUV）的方式来优化下游任务。关于优化MUV的方法，又有搜索求解和VOLT（转化为Optimal Transport问题）两种方式，后者在效果接近的前提下大大节省计算量，更加低碳 MUV 词表大V对于下游任务的影响：V越大，预料的熵越低。但是V越大预测的时候就越难，数据稀疏，造成模型难以学习 MUV和下游任务的BLEU指标关系： 上图可以看到MUV和BLEU的Spearman系数平均值是0.4。可以认定为是正相关的，从而优化MUV是优化BLEU的一种可行方案 优化MUV的方法 MUV-search and Learning 前者是一种基于搜索尝试的方法，复杂度过高 VOLT(VOcabulary Learning approach via optimal Transport) 是一种Learning方法 MUV可以理解为预料在Vocab上的熵对于Vocab大小V的导数（在离散情况下）注意 前文分析MUV和BLEU正相关，并不是Entropy和BLUE正相关，所以我们要求MUV的最大值，而不是MUV为0的情况（这个和边际效益的应用有些不太一样） 方法大致是将V的上限固定在S={k,2k,3k,….}的一个超参数k决定的有限集合内 对于S中每个固定的V上限，将问题转化为char to token 的Optimal Transport问题，用Sinkhorn算法求解出MUV的同时构建出最优的Vocab 再在整个S上找出最优的MUV和相应的Vocab 实验 用相对更小的Vocab size 获得近似甚至更好的BLEU值 在语料系数的数据集上获得更优的结果 除了Transformer结构的，在其他结构的模型里也取得很好的结果 总结 ACL2021最佳论文 一个比较通用的方法，能够没有太大代价的应用在所有NLP任务上。计算一个较小且效果好的Vocab 切入点很新颖，和Marginal Utiltiy 以及Optimal Transport等问题结合在一起 有一些typo和数学推导方面的错误问题，读起来有些困难 相关文档 知乎上一个不错的详细解读 paper实现","link":"/2021/09/29/nlp/VOLT/"},{"title":"《attention is all you need》","text":"论文链接 博客：https://jalammar.github.io/illustrated-transformer/。 原理和结构图在论文以及上面的博客讲的都很清楚，下面提一些我自己阅读论文和博客时遇到的一些疑问，以及后来自己理解觉得对的答案。有些依然没有找到答案。。。 self-attention 做完key和query向量的点积之后，要除以向量维度的平方根，这样可以保持梯度比较稳定 为什么一定要有一个value向量，不能直接用原始向量替代么？ value向量可以表示该token可以被分享到其他token的特征，和该token的embedding不一定一样。而且如果直接用原始embedding作为value的话，self-attention只是等价于之前embedding的重新打散、组合 在做完multi-head之后，把多个head的embedding concat之后还要再接一个Dense层，转化成更低维度传给FFN position embedding 只加在encoder 和decoder最底层的输入中么？ 是的 作用是什么？引入token之间的位置信息：attention机制不能感知token在序列中的位置距离 具体encoding的方案： 这是一个基于token在序列中相对位置做的embedding，所以可以适应任意长度的输入 FFN层 FFN层的作用是什么？每个block除了要做attention，还需要做一些特征变换，在FFN层完成这些事 feedforward层的权重是跨encoder decoder block共享的么？ 不是 residual 每个self-attention层、FFN层、query-attention层都是有residual的 避免梯度消失 原始embedding和过了layer之后的tensor相加之后要过一个BathNormalize层 BathNormalize也是有参数的 (θ, std) 现将原始数据归一化到(θ=0,std=1), 然后再变成想要的分布， 参数数=(layer’s dim - 2) decoder decoder层的self attention只能attention到当前token前面的token，后面的token会被mask掉 decoder输出一个softmax的seq GPT等价于使用transformer的decoder模块（没有给定encoder的信息） 训练 输入输出都是可变长的，由于self-attention层、FFN层、query-attention层都是可以并行计算的，所以不同长度的输入不影响计算。所以只要同一个batch的数据seq_len一样就行了 训练是decoder的输入是待生成的句子，输出是待生成的句子token序列向后错一位的序列 预测 decoder端输入一个序列，预测的下一个token就是decoder输出的最后一个embedding encoder只需要编码一次，然后把encoder的各层输出给decoder端。 decoder需要计算N次 git: https://github.com/tensorflow/tensor2tensor","link":"/2019/05/20/nlp/Transformer/"},{"title":"《Supervised Open Information Extraction》","text":"论文链接 目标：构建一个基于监督学习的openie 建模方式：sequence-labeling 输入：token序列+token的POS信息+基于SRL的predicate开头token的信息 输出：BIO方式标注的predicate ARG0 ARG1 ARG2标签 ARG0表示subject ARG1表示object ARG2表示spo的附加条件（比如时间、地点、情景等） 这里的object定义比较灵活，可以不是一个实体 每个token输出一个probability，span的probability由包含的所有token的probability相乘得到。作者验证相乘的方式是最好的计算span probability的方案 数据集构建： 基于QA-SRL任务数据集转换 基于QAMR任务数据集的转换 openie和QA-SRL的区别：SRL的predicate通常是单个动词，openie则更丰富，可以是多个词 评价方式：利用更宽松的评价指标，只要s,p,o包含对应SRL的头token就可以了 结论： 大多数openie依然存在低recall的问题 提供一个Spo的probability值，让模型的使用者根据具体业务选择threshold是很重要的一个特性 引入QA-SRL的信息对做openie的任务很有帮助 引入QAMR任务的数据、信息到openie任务中，是一个未来发展的方向 git地址：https://github.com/gabrielStanovsky/supervised-oie","link":"/2020/07/02/nlp/supervised-openie/"},{"title":"《Neural Open Information Extraction》","text":"论文链接 目标：从输入文本中抽取schema-free的spo三元组 模型： encoder-decoder的seq2seq模型 原文输入encoder，得到一个encoded embedding 目标序列格式为subjectpredicationobject 引入copy机制，从生成的token和copy的token中选择一个 architecture: 实验： 数据 训练数据从wikipedia的dump构建，36,247,584 pairs,地址：https://1drv.ms/u/s!ApPZx_TWwibImHl49ZBwxOU0ktHv 测试数据：3200 sentence with 10369 extractions https://www.aclweb.org/anthology/D16-1252.pdf 比较对象：OpenIE4(一个基于规则的提取器) 结果：更高的AUC 启发： 做openie的工作，不仅仅可以用NER+NRE的传统方案，还可以考虑seq2seq的方案 seq2seq需要引入copy机制确保生成原文中出现的词语 这种方案可以解决predicate不是原文中连续的一段文本的情况 可以引入transformer来做生成，提高效果 在做decode的时候，可以引入图谱embedding，ner embedding，提高效果 后续可以改进做一个句子中有多个SPO的抽取","link":"/2020/07/02/nlp/neural_openie/"},{"title":"《TPLinker:Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking》","text":"论文链接 解决的问题给定schema的SPO抽取：从文本中抽取去SPO(Subject-Predicate-Object)三元组。其中Predicate是事先定义好的关系，Subject和Object是文中的span TPLinker的特点 能够处理SEO(SingleEntityOverlap)和EPO(EntityPairOverlap)两种情形 SEO：张三和李四都是北京人 -&gt; (张三,出生地,北京),(李四,出生地,北京) EPO：江苏的省会是南京 -&gt; (江苏,包含,南京),(江苏,省会,南京) Single-stage的方案，原始文本过一次Encoder之后，便可以解码得到整个spo三元组 消除exposure bias 这个是重点 pipeline式的抽取模型会有错误传递 https://kexue.fm/archives/6671 为代表的联合训练抽取S和O的方案，在训练时，预测O依赖的是真实的S信息，在预测时，预测O依赖的是预测出的S信息，造成exposure bias 生成式方案:在训练时预测下一个token依赖的是真实的token，预测时预测下一个token依赖的是上一步预测出的token 这些方案都会造成在真实预测的效果和在dev集上评测的效果有较大偏差 TPLinker一步预测出SPO三元组，避免了信息依赖，从而避免了exposure bias TPLinker的实现方案 对每个token pair做分类。对于token pair (t1,t2)，我们只考虑t1出现在t2前面的情形，因而长度为N的序列可以构建出一个长度为N*(N+1)/2的token pair序列 对于每个实现给定的REL,构建两个label:REL-HH, REL-TT分别表示REL的subject_head和object_head的token pair 以及REL的subject_tail和object_tail的token pair 添加一个额外的label:HT,表示subject或者object的head_tail token pair 因此，对于K个实现给定的关系，我们能构建出2*K+1个分类标签 对于左图三元组(New York City, mayor, DeBlasio) (New, City)的HT标签为1 (De, Blasio)的HT标签为1 (New, De)的mayor-HH标签为1 (City, Blasio)的mayor-TT标签为1 对于右图三元组(De Blasio, born in, New York City) HT标签同理 (De, New)的 born in-HH标签为1。由于我们的token pair标志t2在t1后面的情形，所以转化成(New, De)d的born in-HH标签为2 模型结构 解码：HT, HH, TT的标注方式已经可以唯一定位SPO，并且可以表示SEO和EPO的情形。只需要先解码HT标签，找出所有实体的span之后，对于每个REL，解码HH, TT标签后再到实体span中check suject和object范围是否在实体span的列表中即可 实验 总结 Single-stage的解码方式可以很好的避免训练过程中的exposure bias。TPlinker提出了一种编码方式，也可以考虑其他的编码方式来做 Single-stage一般都需要对token pair做排列组合的预测，序列长度平方式增长，因此比较适合短序列的方案","link":"/2020/07/12/nlp/tplinker/"},{"title":"《A Survey on Deep Learning forNamed Entity Recognition》","text":"论文链接 NER是信息抽取、问答系统、机器翻译的一项基础工作，DNN的应用让NER任务有了长足的进步 NER分为两类coarse-grained NER:比较粗粒度的划分entity，比如通用NER。 fine-grained NER：更加细分的实体类型，通常是和具体的业务相关的实体，一个mention可以属于多个实体类别 数据集：见原文table1。 比较常用的有： OntoNotes：18 coarse entity type consisting of 89 subtype CoNLL03 4 entity types 工具：见原文table2 StanfordCoreNLP/NLTK/spaCy 评价指标： exact-match evaluation： 用全匹配方法计算F1。会有些偏严，指标偏低 relaxed-match evaluation: 宽松匹配方案，不太好控制 传统方法： 基于规则的方案 Unsupervised方案：用clustering的方法学习一些统计信息，帮助规则的方案识别NE Feature-Based supervised learning: 基于手工特征提取 DNN方法： Distributed representation word-level: Word2Vec, fastText Character-level Representation: CNN of Character Hybrid Representation Context Encoder Architecture CNN RNN(Recurrent Neural Networks) Recursive Neural Networks: 在树状空间上递归展开（RNN可以看做recursive NN在横向空间展开的特例） Transformer Tag Decoder Architecture FNN + softmax CRF(CRF是HMM更加泛华的一种形式) RNN： 用RNN以language model的方式解码（输入当前token的encode embedding以及上一个状态的tag），据说当entity_type比较多的时候，比CRF快。但是CRF可以求全局最优解，RNN只是从左到右用greedy算法求解，似乎CRF好一些 Pointer Networks：RNN decoder的结构来划分句子中的span，而后再用分类模型对每个span打上entity标签，不需要BIO标签格式 最常用的搭配:Hybrid Representation+RNN+CRF 其他方向： Deep Multi-task learning for NER joint train NER POS CHUNK joint train NER and NRE train NER as a joint of entity-segmentation and entity classification Deep Active Learning for NER Deep Reinforcement Learning for NER Deep Adversarial Learning for NER Deep Attention for NER Challenges Data Annotation Informal Text and Unseen Entities Feature Directions Boundary Detection Joint NER and Entity Linking 将DL的NER和辅助信息、辅助方法结合起来 scalability: 现有的模型已经很大了，难以生产中落地 deep transfer learning 训练可以跨语言、跨领域的模型 few-shot学习（MTB的研究方向） 解决domain-mismatch的方式 方便使用的DL-based NER","link":"/2019/11/07/nlp/survey-NER/"},{"title":"《A Survey on Open Information Extraction》","text":"论文链接 information extraction从文本中抽取出SPO三元组,传统的information extraction都是抽取事先给定的关系 Open information extraction(Open IE)的关系无需实现给定，能够自动从大量的文本中发掘出关系(关系可能是原文中的span，也可能不是) OPEN IE的三个挑战： Automation：需要手动标注的数据必须限制在较小的数量级 Corpus Heterogeneity:能在不同分布的数据集上work，不能依赖领域相关的信息，比如NER。只能用POS这些浅层tag Efficiency：需要在大量数据上运行，需要预测性能高，只能依赖POStag这些浅层信息 OPEN IE的方法： Learning-based System: TEXTRUNNER/WOE/OLLIE Rule-based System:利用语言学、统计学特征+规则 PredPatt Clause-based System: Stanford OpenIE Systems Capturing Inter-Proposition Relationships: 同时抽取三元组以及原文中三元组成立的前提 Evaluation: 目前还没有特别统一的测评数据集以及测评方案 Analysing Errors of Open Information Extraction System 在做通用测评数据、指标的尝试 Open Research Questions: 标准测评数据集以及指标 英语之外的其他语言的openIE system 指代消解技术的引入 规范化定义relation和arguments的方案 基于NN的Open IE方案: Neural Open Information Extraction","link":"/2020/01/17/nlp/survey-OPENIE/"},{"title":"《上帝掷骰子么》书摘","text":"豆瓣链接 这是一本让人相间恨晚的好书。深动形象地将量子理论的发展脉络展现出来（什么时候我能在自己的专业领域做到这样生动的讲解，那便是一大进步了）。量子理论是理论物理的最前沿，也在实践中发挥出了巨大的作用。本篇读书笔记摘抄一些原文并附上一些自己粗浅的理解（极有可能是错误的，毕竟波尔说过“谁认为自己搞懂了量子理论，谁就并不懂量子理论”）。本书值得一刷再刷！ 1900年12月14日这个日子，这一天就是量子的诞辰 站在20世纪的开头，对整个20世纪的科学发展方向起到了重要的决定作用。 之诺悖论：一个人无论如何无法追上一直乌龟。 小学的时候曾经困扰过我。大学学了极限理论之后在数学上证明了只是在一个固定时间之内无法追上。量子论从显示世界无法无限分割的新角度攻破了这个悖论 科学巨人们参与了推动它(量子论)的工作，却最终因为不能接受它惊世骇俗的解释而纷纷站到了保守的一方去 对于第一代的推动者惊世骇俗，对于第二代的科学家可能就没那么惊世骇俗了（在他们学生期间就了解了这个惊世骇俗的理论）。进过许多代科学巨人的推动，巨石最终被推动，并被每个平凡的人们接受。 在卢瑟福的模型中，原子这个极小的体系和太阳系这个极大的体系之间居然的确存在许多相似之处。两者都有一个核心，这个核心占据着微不足道的体积（相对整个体系来说），却集中了99%以上的质量。人们不禁要联想，难道原子本身是一个“小宇宙”。 这种观点被称为“分形宇宙”（FractalUniverse）模型。在它看来，哪怕是一个原子，也包含了整个宇宙的某些信息，是一个宇宙的“全息胚”。所谓一沙一世界。 难道是小宇宙一词的起源？不过这个奇妙的现象确实让人想象有一个终极的定律定义者宏观与微观的一切 爱因斯坦只是说，没有一种能量信号的传递能超过光速 相对论加深理解 德布罗意是有史以来第一个仅凭借博士论文就直接获取科学最高荣誉——诺贝尔奖的例子 出道即巅峰 哥本哈根和哥廷根、慕尼黑一起成为量子力学发展史上的“黄金三角” 20世纪初的科研中心还远不在美国。科技人才的流动性、大师的感召力，能够让科研中心迅速转移。似乎比产业转移更加快。 第五章 part3 作者介绍海森堡发现的矩阵乘法的时候用了乘坐地铁的例子，很形象。给了矩阵乘法一种很形象的现实比喻。作为大师的海森堡没有学过矩阵乘法，波恩学过矩阵乘法却忘了，哈哈。不过海森堡能再次“发明”矩阵乘法，大师风采！ 在高高的石崖顶上，海森堡面对着壮观的日出景象，他脚下碧海潮生，一直延伸到无穷无尽的远方。是的，他知道，这是属于自己的时刻，他已经做出了生命中最重要的突破，而物理学的黎明也终于到来 你怎么能‘看到一个小球的位置呢？总得有某个光子从光源出发，撞到这个球身上，然后反弹到你的眼睛里吧？关键是，一个经典小球是个庞然大物，光子撞到它就像蚂蚁撞到大象，对它的影响小得可以忽略不计，绝不会影响它的速度。正因为如此，我们大可以测量了它的位置之后，再从容地测量它的速度，其误差微不足道 测不准原理，实验物理学的一大难题。虽然导致量子力学很多实验难做，但是不影响人们做思维实验，理论推导，再应用到实践中。 共轭量：精准测试一个变量，另一个变量就变得无法精准获得。 比如时间和能量，位置和速度 玻尔的“互补原理”（The Complementary Principle）连同波恩的概率解释，海森堡的不确定性，三者共同构成了量子论“哥本哈根解释”的核心，至今仍然深刻地影响着我们对于整个宇宙的终极认识 谈论任何物理量都是没有意义的，除非首先描述你测量这个物理量的方式。一个电子的动量是什么？我不知道，一个电子没有什么绝对的动量，不过假如你告诉我你打算怎么去测量，我倒可以告诉你测量结果会是什么 奥卡姆剃刀原理是一个叫威廉的修士提出的。奥卡姆是他出生的地方 自海森堡取得突破以来，理论物理进入了前所未有的黄金年代，任何一个二流的学生都可能在其中做出一流的发现虽然量子论的解释依然百家争鸣，没有一个定论，但是不妨碍它成为有史以来实践应用中最成功的物理理论 和编程调包类似，虽然看不懂源码，或者看懂源码不知道为什么这样做。并不妨碍我们调包作出伟大的实践。在实践中的成功又让人们反过来试图解释量子理论而不是质疑量子理论 薛定谔的猫：本来哥本哈根解释只在微观领域，虽然叠加状态很让人费解，但是不影响人们对宏观事物的理解。薛定谔用一个装置巧妙地把这种叠加状态转移到宏观的猫身上，这下大家就得挠破头皮地解释这只猫了😂 多宇宙解释：我们的宇宙其实是在多种叠加状态中的，每个确定的“你”生活在这个叠加宇宙的一个投影中。由于维度非常高，两个投影之间的相关度几乎为0。所以不同的宇宙投影可以认为是不相干的平行宇宙。 这个解释是我对量子论所有解释中最认同的一个。同时一些小概率的投影之间相干是否能够用来解释某些灵异事件？ 科学的一个特性是能被证伪。严格意义上来说，证伪和证实一样是不可能完成的任务。当出现一个反例，你可以说是测量的问题，可以说是问题定义的问题，甚至创造一个不存在的概念来兼容这个反例，有点类似撒泼打滚。有趣的是，在真正的科学史上，往往还都是这种“撒泼打滚”的情况居多。 因此，单凭列举“反面证据”，根本就不可能证伪牛顿理论。事实上，如果仔细考察科学史，我们就会发现，几乎没有任何理论是因为“被证伪”而倒台的，它们退出历史舞台，几乎只有一个理由，就是出现了一个更好、假设更少、更合理的新理论。 量子自杀实验：由于多个平行宇宙，在你有意识的那个宇宙中，你一定没有死，无论连续开枪多少次。你的生死决定了你在哪个宇宙，而不是给定一个宇宙，判定你的生死。 换句话说，同样是读入10bits的信息，传统的计算机只能处理1个10位的二进制数，而如果是量子计算机，则可以同时处理210个这样的数！ 叠加性让量子计算机可以同时处理所有的可能性。bit数越多，量子计算机的效率优势越明显，而且还是指数级的！ 贝尔不等式是判定爱因斯坦支持的决定论和波尔支持的量子论的最终判定条件（因为其可以在现实中做实验），最终波尔取得了最终的胜利。（不知道如果爱因斯坦还在世的话，能想出什么法子“找茬儿”，哈哈） 不管是牛顿还是爱因斯坦的理论，都是时间对称的 你可以用这些理论推算历史，逆着时间的河流探索。而现实中，时间只能向前 最新的进展：万能理论（Theory ofEverything, TOE）。统一宇宙中的四种力：：引力、电磁力、强相互作用力和弱相互作用力 万物理论的整体框架","link":"/2021/11/24/physics/%E4%B8%8A%E5%B8%9D%E6%8E%B7%E9%AA%B0%E5%AD%90%E4%B9%88/"},{"title":"局部敏感哈希(LSH)与文本去重","text":"本文旨在搞清楚哈希函数、局部敏感哈希、MinHash、SimHash之间的关系。对利用局部敏感哈希来做最近邻查找的问题做一个梳理和总结。本文主要参考stanford公开课cs246的课件，讲得非常清晰，要系统的理解一个问题，还是得看这种课件，比网上搜索的碎片化信息有用多了。课件链接在文末的参考文档中,文中的截图均来自课件。 目标问题 在高维空间里寻找相似的条目 寻找相似的文章（查重） 寻找相似的商品（电商推荐） 寻找相似的图片 （以图搜图） 数学定义 在高维空间的N个点X={x1,x2,…x,n}中，寻找所有和目标点x 的距离d(x, xi)小于给定阈值T的点。 目标 O(N) -&gt; O(1) 距离与相似度 距离定义： 例子:给定两个点 x=(0,1,2,3,4), y = (0,1,-2,0,4) jaccard相似度: len({0,1,4})/len({0,1,2,3,4,-2}) = 3/6 jaccard距离： 1-jaccard相似度(从相似度定义衍生出来的距离，用来比较两个set。和维度的大小以及顺序无关) cosine距离：arccos(x·y/|x|·|y|)/π 欧氏距离： sqrt(0+0+16+9+0) = 5 曼哈顿距离: 0+0+4+3+0 = 7 汉明距离： 0+0+1+1+0 =2 距离与相似度 一般定义相似度=1-距离，但这两者有两个细微的区别 相似度取值范围 [0,1], 距离可以定义在[0, inf]上 距离函数需要满足三角不等式，相似度没有这个限制 总体思路 利用hash函数，将相似的点分配到同一个bucket(相等的hash值)。 然后对给定的点x，只需要在其对应的buket中比较。我们希望hash函数有如下两条性质，这样的hash函数被称为局部敏感哈希(LSH) 每个bucket中尽量含有少的点（减少False Positive，不相似的点尽量不要在同一个bucket中） 尽量减少False Negatives：（相近的点没有分在同一个bucket中，从而被遗漏掉） 形式化定义 Hash 函数 一系列函数，用来判断两个object是否相等 if x=y, hash(x) = hash(y)。 反之不一定 不同应用场景下需要不一样的特性 hashmap：hash的结果尽量在值域上均匀分布，减少hash冲突 MD5等加密场景：减少冲突的同时不可逆计算 局部敏感hash：原数据相近，大概率hash值相同 查重实战给定一篇文章，在海量候选文章中找到与其相近的文章。（相近的定义取决于编码方式以及距离函数的选择）。主要分为一下三步 shingling:将文章编码成高维向量 min-hasing:利用LSH计算文章的哈希值 优化S-Curve shingling k-shingle(n-gram)切分 n越大，编码维度越高 可以用jaccard相似度定义文档相似度, 例如 A=abcabdd, B=abdadd, k=2 A={ab, bc, ca, bd, dd}, B= {ab, bd, da, ad, dd} Sim(A,B) = len({ab, bd, dd})/len({ab, bc, ca, bd, dd, da, ad}) = 3/7 N个文档可以转化为一个N列M行(M为所有文档的shingles的并集大小)的矩阵（非常稀疏） 如下图，4片文档，7个不同的shingles。1表示该文档包含该shingle，0表示不包含min-hashing min-hash是一种局部敏感hash 对于上一步得到的矩阵，随机选择一种行的排列方式π, 对于列C, hπ(C)=”C在π的排列方式下1所在的最小行号” 所有这样的hπ函数都称为min-hash 两列的min-hash值相等的概率等同于两列的jaccard相似度 Pr[hπ(C)=hπ(C)] = Simjaccard(C1,C2) 证明： 经过π重新排列，C1,C2在某一行i的取值有如下四种情况 a. C1i = 0, C2i = 0 b. C1i = 1, C2i = 0 c. C1i = 0, C2i = 1 d. C1i = 1, C2i = 1 考虑C1或者C2的值为1的最小行r, r属于(b,c,d)三种情况之一 其中r属于情况d的时候hπ(C)=hπ(C) 因此Pr[hπ(C)=hπ(C)] = |d|/|b|+|c|+|d| = Simjaccard(C1,C2) 由于Pr[hπ(C)=hπ(C)] = Simjaccard(C1,C2)，hπ是一个局部敏感hash 下图展示了三个不同π排列方式下，minhash的取值 优化S-Curve 对于一个局部敏感哈希函数h，我们可以画出Pr[hπ(C)=hπ(C)] 随着Sim(C1,C2)变化得到的曲线S-Curve （注意这里的Sim不一定是jaccard相似度） 如图所示，我们希望 Pr[hπ(C)=hπ(C)] 和 Sim(C1,C2)是正相关的 给定一个相似度阈值s(图中红线) 黄色的区域是FN:False-Negative(相似，但是hash值不等) 蓝色区域是FP:False-Positive(不相似，但是shah值相等) 我们希望两块区域都越小越好。根据不同的业务场景，有时降低False-Negative重要，有时降低False-Positive重要 我们可以引入F-Value(1-FN-和1-FP的调和平均)来计算S-Curve的的优劣。F-Value越大，S-Curve越好 min-hash的S-Curve 显然min-hash的S-Curve不是很好（FN和FP都太大了） 我们希望用一些方法让S-Curve变得更加陡峭一些 BR优化法（我自己取得名字😂） 这是一个通用的优化方法，不仅仅可以用于min-hash 随机获取M个π，用hπ作用到C上M次，将C转化为一个M维的0/1向量 将长度为M的0/1向量分为B组，每组R个数字（B*R=M） 对于C1和C2,如果他们任意一个组内的R维向量相等，认为C1和C2相似 B越大，FN越小，R越大FR越小 试着调整 B,R的值，看看对Scurve的影响：https://www.desmos.com/calculator/lzzvfjiujn?lang=zh-CN 后文我们会介绍:BR方法本质上是在组内进行AND操作，在组之间进行OR操作 通过优化BR的值，我们可以获得一条在给定阈值T的情况下，FN和FP都比较小的S-Curve曲线 更详细的定义对于LSH，我们可以给出如下更详细的定义，引入d1,d2,p1,p2四个参数： 下面列举不同距离定义下LSH函数，以及他们的参数 minhash 距离定义：jaccard距离 给定一种排列方式π，hπ(C)=”C在π的排列方式下1所在的最小行号” Pr[h(x) = h(y)] = 1 - d(x, y) (d1, d2, (1-d1), (1-d2))-sensitive random hyperplanes hash 距离定义：cosine距离 给定一个随机向量v，hv(x) = +1 if v·x &gt;= 0 else -1 Pr[h(x) = h(y)] = 1 – d(x,y) / π (d1, d2, (1-d1/π), (1-d2/π))-sensitive 详细参考stanford课件(4.lsh-theory)45页 projection hash 距离定义： 欧氏距离 给定一个向量l，将其分成n个长度为a的bucket，hl(x) = x在l上的投影所属的bucket if d &lt;&lt; a pr[hl(x) ==hl(x) ] &gt;= 1-d/a if d &gt;&gt; a pr[hl(x) ==hl(x) ] &lt;= 2arccos(a/d)/π (a/2, 2a, 1/2, 1/3)-sensitive 详细参考stanford课件(4.lsh-theory)51页 AND-OR 变换 对于单个LSH，S-Curve不够陡峭，FN和FP都太大 随机应用多个LSH， 得到长度为M的signature 将M分为B个组，每个组长度为R，B*R=M 组内做R次AND操作，组间等同于B次OR操作 通过如上操作，可以变更LSH的参数，从而陡峭化S-Curve (d1, d2, p1, p2)-sensitive-&gt; (d1, d2, 1-(1-p1R)B, 1-(1-p2R)B)-sensitive SimHash SimHash是谷歌2007年论文《Detecting Near-duplicates for web crawling》提出的算法，用来做文本查重。网上有很多关于SimHash具体算法介绍的文章，这里就不再赘述 看完哪些文章，一直知其然不知其所以然。知道看了Stanford关于LSH的课件之后，经过 深入理解simhash原理博客的点拨，才明白simhash其实也是上文框架下的一种具体文本查重解决方案。 SimHash本质上是针对Cosine距离的一种random hyperplanes hash算法应用。下面把SimHash的步骤和本文介绍的LSH在文本去重中应用的框架对应起来 分词：shingling操作，SimHash给每个shingle增加了一个权重，这个权重只是一种优化手段，我们可以把权重都当做1，不影响对SimHash的本质理理解 将每个shingle hash成一个M维的0/1向量：本质上是生成了M个随机向量。每一列都是一个随机向量 对于每一列求和：求随机向量和文本向量的点积 计算两个文档的汉明距离，认为汉明距离小于d为相似：M个hash值分为(d+1)组，每组M/(d+1)个数，然后利用BR优化法优化S-Curve 总结解决在海量数据（文本、图片、商品）中快速寻找相似数据的问题 将数据向量化（通常得到一个高维、稀疏的向量） 定义一个距离函数，以及一个阈值T，定义距离小于T为相似 根据距离函数，找到一个(d1, d2, p1, p2-sensitive)的LSH函数 随机选择LSH的参数，应用LSH函数M次，将向量转化为长度为M的hash值 选择一组B和R的值。B*R=M， 从而得到一个(d1, d2, 1-(1-p1r)b, 1-(1-p2R)B)-sensitive 的LSH函数 寻找相似数据的时候，只在最终的LSH函数值相同的分桶中找 这是一个近似算法，需要权衡FN和FP。 FN的数据就被漏掉了，FP的数据可以在分桶中检查时去除，但是FP过高也会降低算法的效率 参考资料 stanford课件(3.lsh) stanford课件(4.lsh-theory) 深入理解simhash原理","link":"/2021/11/20/computer/LSH/"},{"title":"《穷爸爸富爸爸》书摘","text":"豆瓣链接 税是惩勤奖懒 如果有事情必须被改变，首先要改变的是自己 穷人和中产阶级为钱而工作。富人让钱为他工作 要对现状感到一定程度的愤怒。 激情是愤怒和热爱的结合体 你挣钱时得交税，花钱时也得缴税。你存钱时得缴税，你死时还得缴税 以你喜欢的方式运用你的头脑和感情，不要让它们控制你 随着金融衍生品的出现和经济形式的日益复杂，支配金钱已经成为在世界经济中幸存的必要条件 只有知识才能解决财务问题。那些不是靠财务知识挣来的钱也不会长久 富人获得资产，穷人和中产阶级获得负债，只不过他们以为那些负债就是资产 我的顾问团队是我最大的资产之一 买房的最大损失是致富机会的损失 富人关注资产而其他人关注收入 麦当劳是世界上最大的独立房地产商了 从事你所学的专业的可怕后果在于，他会让你忘了关注自己的事业 事业是我拥有他们，可以交给别人管理和经营。 如果我必须在那儿工作，那就是我的职业而不是我的事业了 富人最后才买奢侈品 那些刺激经济的政策降低了富人的税收。所以政府只能从中产阶级那儿获得税收了。这些税收又通过政府采购的方式支付给了富人 在现实生活中，人们往往是依靠勇气而不是智慧去取得领先地位的 如果投资机会太复杂我又弄不明白，我就不会去投资 如果你清楚自己在做什么，就不是在赌博。如果你把钱投入一笔交易，然后只是祈祷，那才是赌博 大部分人需要学习和掌握的不止一项技能 要选择那些真正做过你想做的事情的人当你的老师 我知道的东西带给我金钱，我不知道的东西使我失去金钱 人们总是匆匆忙忙去赶那些已经过去的浪头，往往又会被新的浪头淘汰出局 变富有的关键是拥有尽快将劳动性收入转化成被动收入或投资组合收入的能力","link":"/2015/09/15/finance/%E7%A9%B7%E7%88%B8%E7%88%B8%E5%AF%8C%E7%88%B8%E7%88%B8/"},{"title":"Kylin, Mondrian, Saiku系统的整合","text":"本文主要介绍有赞数据团队为了满足在不同维度查看、分析重点指标的需求而搭建的OLAP分析工具。这个工具对Kylin、Mondrian以及Saiku做了一个整合，主要工作包括一些定制化的修改以及环境的配置。目前这个系统还处于一个需要优化、完善的过程，这篇博文也会相应地更新。 背景在有赞发展的初期，数据团队主要的工作之一就是根据运营人员的报表需求，编写sql，从hive中获得数据并写入mysql中存储。最后，前端人员写相应的代码展现mysql中存储的报表数据。随着公司业务的快速发展，如此长周期的报表开发流程已经很难跟上运营人员的分析需求了。为了避免深陷报表开发、维护的泥潭，数据组决定调研大数据场景下的OLAP分析工具。参考了明略数据的解决方案之后，我们选择整合Kylin，Mondrian，Saiku来实现这样一个OLAP系统。 三巨头Kylinkylin是apache软件基金会的顶级项目，一个开源的分布式多维分析工具。下面是摘自Kylin官网的介绍： Apache Kylin™ is an open source Distributed Analytics Engine designed to provide SQL interface and multi-dimensional analysis (OLAP) on Hadoop supporting extremely large datasets, original contributed from eBay Inc. 个人的理解是：Kylin通过预计算所有合理的维度组合下各个指标的值并把计算结果存储到HBASE中的方式，大大提高分布式多维分析的查询效率。Kylin接收sql查询语句作为输入，以查询结果作为输出。通过预计算的方式，将在hive中可能需要几分钟的查询响应时间下降到毫秒级。更细致的关于Kylin的介绍，可以参考我的另一片博客Kylin初体验。 Mondrian Mondrian is an Open Source Business Analytics engine that enables organizations of any size to give business users access to their data for interactive analysis. You can build powerful Business Intelligence solutions with Mondrian as your Online Analytical Processing (OLAP) engine, enabling multidimensional queries against your business data, using the powerful MDX query language. Mondrian是一个OLAP分析的引擎，主要工作是根据事先配置好的schema，将输入的多维分析语句MDX(Multidimensional Expressions )翻译成目标数据库／数据引擎的执行语言（比如SQL）。 Saiku Saiku allows business users to explore complex data sources, using a familiar drag and drop interface and easy to understand business terminology, all within a browser. Select the data you are interested in, look at it from different perspectives, drill into the detail. Once you have your answer, save your results, share them, export them to Excel or PDF, all straight from the browser. Saiku提供了一个多维分析的用户操作界面，可以通过简单拖拉拽的方式迅速生成报表。Saiku的主要工作是根据事先配置好的schema，将用户的操作转化成MDX语句提供给Mondrian引擎执行。 技术架构Kylin + Mondrian + Saiku是一个简单的三层架构。git上开源的Saiku的项目已经整合了mondrian的jar包。所以构建这样一个三层架构主要的工作是将Mondrian的schema和Kylin的schema对应起来，同时需要针对Kylin的语法对Mondrian做一些Kylin dialect的定制开发。Git上已经有一个整合Kylin，Mondrian以及Saiku的项目。照着这个项目的指引，可以很轻松的搭建这么一个三层的系统。在此，致谢开源项目作者mustangore。 一些细节介绍完整体的结构，下面讲一些构建过程中遇到的坑。有些可能是我们的理解还不够深入，有些可能随着开源软件版本的升级已经不再是一个坑了。希望能给大家带来一些帮助，如果是由于我们理解的偏差导致踩到的坑，也希望大家留言给出指正：）本套系统构建基于kylin1.5， Mondrian4.4以及Saiku3.7.4。底层是Hive0.14以及Hbase0.98。 关于schema前面提到，要让系统运转，Kylin的schema必须和mondrian的schema能够对接上。Kylin是根据自身cube配置的schema来进行预计算的，schema决定Kylin能够接收的sql查询的范围。Mondrian又根据自身的shema翻译MDX到sql， Mondrian的schema决定它生成的sql的范围。如果两者有不一致的情况，就可能导致Mondrian生成的sql无法被Kylin执行。kylin的schema配置比较简单，管理页面上有一套图形界面指引你一步步地构建一个星型模型，配置di mension、measure。不过要把cube设计得高效，Kylin还是有不少高级地设置的，比如选择 attribute group， derived dimension等。官网上有详细的介绍。Mondrian的schema没有比较好的图形配置工具，需要手写Mondrian schema的XML文档，文档格式参考官方文档，通过Saiku上传。需要注意的坑： 不要用view作为lookup table 在设计Kylin cube时，用hive view作为fact table是一个比较好的实践方式，可以屏蔽一些底层数据结构变化对Kylin cube的影响。但是不要用view作look up table，在build cube计算维度表容量时会出问题。 Kylin无法在预计算指标时制定条件 比如有两个字段：order_pay, is_payed。我们可以配置sum(order_pay)作为订单金额, sum(is_payed)作为付款订单数。但是没法配置sum(order_pay) where is_payed = 1来表示付款订单金额。我们需要在fact view中添加字段payed_order_pay表示付款的订单金额。 尽量在Kylin中用int类型 比如is_payed字段，就0/1两个值，通常我们在hive里可以设置为tiny int类型的字段。但是在Kylin中，针对tiny int 和 int类型的字段配置出来的measure类型是不一样的，tiny int 类型的字段得到的measure在和Saiku结合时可能会出现问题。 把hive表放在default库中 Kylin添加hive table的时候是可以指定hive table所在库的，但是建议将fact table、lookup table都放在default库中。因为在Mondrian的schema中，physical table是默认去default库查找的，目前还没有发现很好的在Mondrian schema中指定数据库的方式。 关于count distinctKylin配置cube的时候可以指定某个measure的聚合方式为count distinct，有精准计算的方式也有基于hyperloglog算法的近似计算方式。同样，在Mondrian的schema里也可以配置count distinct的指标聚合方式。看上去一切都OK，然而问题来了：Kylin的count distinct语法只针用count distinct聚合的指标字段，在计算维度表大小的时候，kylin无法计算类似 select count(distinct date) from lu_date这样的sql语句。在mustangore的项目中，对Mondrian打了Kylin-dialect的补丁。其中添加了一个JdbcDialect的实现： public class KylinDialect extends JdbcDialectImpl { public static final JdbcDialectFactory FACTORY = new JdbcDialectFactory(KylinDialect.class, DatabaseProduct.KYLIN) { protected boolean acceptsConnection(Connection connection) { return super.acceptsConnection(connection); } }; /** * Creates a KylinDialect. * * @param connection Connection * @throws SQLException on error */ public KylinDialect(Connection connection) throws SQLException { super(connection); } @Override public boolean allowsCountDistinct() { return false; } @Override public boolean allowsJoinOn() { return true; } } 注意到：allowsCountDistinct()函数被设置成了return false；mustangore 通过这种方式避免了Mondrian计算维度大小的时候count disctinct，然而这种一杆子打死的方式也使得Mondrian计算count distinct的指标的时候出现问题：select count(distinct XXX) from tableA这样的语句会被翻译成select count YYY from (select distinct XXX as YYY from tableA)，而Kylin又不能很好的执行后者。为了解决这个两难的问题，我们深入到Mondrian的源码中去，找到了计算维度表大小的代码： private static String generateColumnCardinalitySql( Dialect dialect, String schema, String table, String column) { final StringBuilder buf = new StringBuilder(); String exprString = dialect.quoteIdentifier(column); if (dialect.allowsCountDistinct()) { // e.g. \"select count(distinct product_id) from product\" buf.append(\"select count(distinct \") .append(exprString) .append(\") from \"); dialect.quoteIdentifier(buf, schema, table); return buf.toString(); } else if (dialect.allowsFromQuery()) { // Some databases (e.g. Access) don't like 'count(distinct)', // so use, e.g., \"select count(*) from (select distinct // product_id from product)\" buf.append(\"select count(*) from (select distinct \") .append(exprString) .append(\" from \"); dialect.quoteIdentifier(buf, schema, table); buf.append(\")\"); ... 注意到只有dialect.allowsCountDistinct()为true时才会用count distinct来计算维度表大小。我们只要将Kylin dialect的allowsCountDistinct()设置为true，同时在generateColumnCardinalitySql添加一个判断条件： if (dialect.allowsCountDistinct() && !dialect.getDatabaseProduct().name().equalsIgnoreCase(\"KYLIN\")) { ... 就可以实现和kylin的count distcint measure的正常对接了。 关于Kylin sql有了处理count distinct的问题的经验，我们发现，只要了解Kylin sql的特点，针对Kylin sql定制Mondrian 的Kylin—diect就能将Mondrian和kylin较好的对接。经过在Kylin1.5的交互界面中的测试，我们列出如下的区别： 不能limit beg, end 只能limit length 不支持 union, union all 不支持 where exists 子句 结束语以上是有赞数据团队实现多维分析工具的探索过程。总的来说，Kylin ＋ Saiku ＋ Mondrian的一套流程是能走通的，中途遇到一些零碎的问题没有完全列出来。通常是因为Kylin只支持cube范围内的查询，如果Mondrian翻译出的sql超出这个范围就会引起系统的错误。通常有三种解决方案： 重新构建Kylin cube，让它能覆盖更广范围的查询 修改Mondrian schema，让它的cube描述和Kylin cube吻合 定制化开发Mondrian的Kylin dialect，让Mondrian生成符合Kylin特点的sql 目前我们还在对这套三层框架做一些定制化的功能开发以及性能的调优工作。希望这篇文章能给大家带来些帮助，也希望有独特见解或者发现我们的理解不对的朋友们留言交流。","link":"/2016/04/19/bigdata/KMS/"},{"title":"《彼得林奇的成功投资》书摘","text":"豆瓣链接 进入2017，决定开启我的投资之旅。之前也读过一些关于经济、金融方面的书：《经济学通识》、《金融的解释》、《富爸爸、穷爸爸》。真正的投资一直没有开始，总觉得自己还没有准备好。新的一年开始，决定做一把行动派，把一些手头的闲钱投入股市。作为多年读书的“书呆子”，自然还是要买一本经典放在枕边稳稳军心的。豆瓣一搜，《彼得·林奇的成功投资》排名股票类的第一名，遂读之。 诚然，4个晚上的迅速读完之后，对价值投资一说有了些了解，可是对于怎么在股市里叱咤风云还是一脸懵逼。 本书的前半部分大致是诚心、正意的内容，后半部分的对实战有些指导意义。可惜是针对上世纪80、90年代的美国的一些经验。作为初学者，没有自动适配新的年代、新的国家的能力， 加上急于上手尝试的心情，后半段囫囵吞枣的读完了。 顺便说一句，2017年入市46天来，亏损9.67%，跑输上证指数12.67% 😂。 好在大师在这本经典里教导大家要心态平稳，目前心态还好，斗志尚存。下面记录一些我个人的在书中总结的技巧（如果各位看官还愿意看我的总结的话）： 留心你的日常生活，对于那些卖的好，正在快速发展的公司，问自己一句：“这是一家上市公司么？”。日常观察是对一个公司基本面判断的最直观方式。 专业的投资者、基金经理选股有很多限制条件，让他们无法看见一些资产很小的、不出名的优质股，这都是我们相对他们的优势。 和一群同行一起工作有一个劣势就是：把个人思考变成了集体思考。自己的独特想法很可能被大众的想法淹没。 基金经理选股比较平稳，散户选股比较容易选中差的股票，但是不要紧，只要在你的投资组合里有一个10倍股就能扭亏为盈。 在投资股票之前，问自己三个问题：1.我有一套房子么？ 2.我有可能急着需要这笔钱么？3.我有投资者需要的素质么？ 投资者的素质：耐心、自立、常识、对于痛苦的忍耐能力、心胸开阔、超然、坚持不懈、谦逊、灵活、愿意独立研究、冷静。 投资者在挑选股票、股票也在挑选投资者。 不研究公司就投资就像不看牌玩德州一样危险。然而很多股民就是这样的，把股市当作赌场。 购买如下六种公司的股票： 缓慢增长型公司：大企业，定期慷慨的支付股息。 稳定增长型公司：过去的历史证明公司比较稳定，对投资组合有保护作用。 快速增长型的公司：不属于快速增长行业的快速增长公司更佳。 周期型公司：关键确定好公司到了周期的哪个阶段。 困境反转型公司：真的反转了么？ 隐蔽资产型公司：常见的隐蔽资产：土地。 没有竞争的行业里的管理水平很普通的公司比一个处于激烈竞争的行业中管理水平非常高的优秀公司更佳值得投资。 公司名字枯燥乏味可能使得投资者用比较便宜的价格买到股票。 避开热门行业的热门股票。 和商品的原理一样， 如果企业是个巨大的商品，购买股票就是购买这个商品的一小片。最终企业的价值决定股票的价格（价值投资的核心）。 最重要的是公司的收益。 坚决不购买市盈率极高的股票，就能避免重大亏损。 下单前沉思两分钟。 拜访公司、阅读年报、通过任何手段获得真实的公司经营数据非常重要。 如果市盈率是15， 那么应该预期收益增长率为15%， 市盈率如果低于收益增长率那就非常完美了。 产品越接近最终制成品，出售价格越难以预期。 大家都知道棉花的价格是多少，但是一件衬衫的价格差异就很大了。 每股现金是股票价格的下限。 不要根据涨跌的百分比来决定自己是否该买入还是卖出。 想要探到一个下跌股票的底，就好像试图抓住一把快速下落的刀子，通常不但抓不到刀子，还会划伤自己的手。 重新整理了一遍读书的心得，才发现自己之所以败的这么惨，不是大师的话完全出错了，而是自己操作的时候完全忘记了这些准则。无论如何，作为我的投资启蒙书，书中关于公司如同商品，价格回归价值的理论说服了我。我打算做一名价值投资的忠实信徒，放眼长线利益。","link":"/2017/02/11/finance/%E5%BD%BC%E5%BE%97%E6%9E%97%E5%A5%87%E7%9A%84%E6%88%90%E5%8A%9F%E6%8A%95%E8%B5%84/"},{"title":"《硬派健身》书摘","text":"豆瓣链接 大肌群决定了人的整体形象，锻炼的时候消耗的能量也最多，所以首先应该从大肌群锻炼开始（胸背臀腿） 从大肌群减脂，才更有效果， 训练时以大肌群+核心集群（腹部、下背部）搭配为好 肌肉密度要比脂肪大不少，体重.BMI不能完全体现身材 静止状态下，肌肉消耗的能量远高于脂肪，促进新陈代谢 体重设定理论：通过节食的方式可以快速减脂，但是身体适应之后，就会减少代谢，让你的体重很难再下降 间歇性高强度有氧训练效果好于持续低强度有氧 无氧训练之后，还会一直燃烧脂肪，所以应该先做无氧，再做有氧 人是耐力最好的哺乳动物 HITT high-intensity interval training HIT的一个不成文规定就是让身体尽可能多的部位活动起来 脂肪只在有氧状态才能燃烧（无氧不能立刻减脂，但是会让之后的减脂的效率提升） 增肌和减脂并行不悖 在健身过程中，心中默念发力部位很有效 长期不动、第一次运动后非常疼痛， 继续运动不会增加疼痛，反而有一定减轻疼痛的效果 增肌者最好在运动前喝一些缓释的碳水或者蛋白饮料 自重训练强度不太好调节，且活动部位比较灵活，不太适合初学者 RM (reputation maxium) 最大重复数量，衡量一个动作的强度 增肌最好要用12左右RM的负荷 深蹲是训练项目之王 力量训练可以通过 30%-40%的重量，20-30一组的多关节多关节复合运动来热身 三大营养素的卡路里意义是不同的，不能单纯计算热量来区别 吸收蛋白质的能量消耗是吸收脂肪的5倍，所以多吃蛋白质为主的食物 增肌需要吃2*体重/1000的蛋白质 我们需要选择人工加工成分少的食物 血糖生成指数GI，描述食物被人消化吸收的速度 低GI的食物比较抗饿 越精致加工的食物，GI越高， GI高于60需要警惕 运动后可以吃高GI的食物，促进身体的吸收 健身活动3h之后，是补充、吸收蛋白质的绝佳时间，可以吃蛋白粉，或者脱脂牛奶、鸡蛋清、各种瘦肉 运动前2h摄入低GI的碳水化合物比较好 练后不吃，视同白练 训练前、不要摄入脂肪，训练中、练后2h 也要远离脂肪 训练计划中可以调整训练负荷， 一版可以 强 弱 中 中 这样的规律","link":"/2020/04/17/organism/%E7%A1%AC%E6%B4%BE%E5%81%A5%E8%BA%AB/"},{"title":"kylin初体验","text":"从去年12月开始，为了提高公司OLAP系统的查询速度，开始接触kylin，前前后后折腾了近三个月。踩了无数的坑，才算是初窥门径。特在此把自己的感悟、理解记录下来，算是重新梳理一边自己的所得，也希望能给众位正在使用kylin或者打算使用kylin的小伙伴们提供一点帮助😊 一句话的概括（我的理解）kylin将OLAP分析的星型模型schema的所有group by聚合结果存储在HBASE的表中，通过将sql查询转化成对hbase表的聚合操作的方式大大提高查询速度。 官方介绍http://kylin.apache.org/Apache Kylin™ is an open source Distributed Analytics Engine designed to provide SQL interface and multi-dimensional analysis (OLAP) on Hadoop supporting extremely large datasets, original contributed from eBay Inc. kylin的角色 上层应用 SQL应用：Zeplin等，通过jdbc/odbc调用kylin 第三方应用：通过kylin的restful接口调用kylin BI应用：Saiku+Mondrian， Tabula kylin 提供jdbc/odbc driver以及restful接口，接受sql查询 判断sql该从hbase直接查询，还是该查询原始的hive表 提供cube设计、构建、管理的接口和UI 下层基础(kylin 1.2) hadoop 2.4 - hadoop 2.7 hive 0.13 - hive 0.14 hbase 0.98 - hbase 0.99 JDK 1.7+ kylin 架构Kylin 架构： REST Server：提供jdbc/odbc接口以及restful接口 Metadata：存储在hbase中的cube相关信息（project信息、hive table 信息、cube Schema…) Routing:处理Rest Server接受到的请求，查询Metadata，判断从hbase取数还是从hive取数 Query Engine：根据Routing 的判断，执行hbase查询或者hive查询，返回数据给REST Server Cube Build Engine：根据metadata中的cube信息，将hive中的数据预计算并写入到hbase kylin build cube的方法在官网上有详细的介绍，这里不再赘述 踩过的坑与感悟 kylin 的cube 对于用户是透明的。用户无法直接从cube查询。用户需要写维表与事实表的join语句， kylin负责转译成cube的查询。这点也是kylin和中间表、宽表的一大区别。（中间表、宽表在用户端就让sql变得简单， kylin在执行时让sql变得高效）因此，kylin适合与bi引擎对接。中间表、宽表事宜提供给用户。 kylin对hadoop生态的版本非常敏感，最好不要超出推荐的版本好范围 搭建kylin时： 需要安装snapy压缩算法，或者将kylin.hbase.default.compression.codec配置为其他已有算法 需要将$HBASE_HOME/conf/hbase-site.xml 中hbasehbase.zookeeper.quorum字段的默认端口2181去除掉 需要用huser用户启动kylin 需要保证kylin服务机器与集群中每台机器的hadoop相关环境变量一致 kylin执行MR任务时如果报错hcatalog class not found.build cube时集群中每台机器都有一些hive的jar包(比如hive-hcatalog-core***.jar)，如果没有，需要合并到$KYLIN_HOME/lib／kylin-jdbc-{version}.jar中。kylin运行时会将这个jar包put到HDFS上。 对kylin的job进行管理时，常常会因为kylin metadata的缓存导致找不到资源id对应的cube、segment，从而产生一个null pointer错误。解决方法：重启kylin清除缓存，或者修改kylin源码，对job操作中可能出现null pointer的地方做相应处理。 kylin的官方介绍中，对于出现在hive表但是没有被预处理存储到hbase中的字段的查询，kylin可以直接查询hive。经过在kylin UI中试验，发现kylin并不能找到hive表，会报找不到字段的错误。 kylin对绝大多数SQL语句都有很好的支持，下面列出经过测试kylin不能很好支持的sql语句： &gt; - limit 后面添加一个起始位置，一个数据长度，比如limit 1， 20 &gt; - right outer join &gt; - left outer join &gt; - select 不在group by 中的字段 &gt; - count(distinct) &gt; - insert, update, delete &gt; - union, union all &gt; - where exists","link":"/2016/03/08/bigdata/kylin/"}],"tags":[{"name":"刘未鹏","slug":"刘未鹏","link":"/tags/%E5%88%98%E6%9C%AA%E9%B9%8F/"},{"name":"书摘","slug":"书摘","link":"/tags/%E4%B9%A6%E6%91%98/"},{"name":"李笑来","slug":"李笑来","link":"/tags/%E6%9D%8E%E7%AC%91%E6%9D%A5/"},{"name":"FIRE","slug":"FIRE","link":"/tags/FIRE/"},{"name":"财富","slug":"财富","link":"/tags/%E8%B4%A2%E5%AF%8C/"},{"name":"凯利·麦格尼格尔","slug":"凯利·麦格尼格尔","link":"/tags/%E5%87%AF%E5%88%A9%C2%B7%E9%BA%A6%E6%A0%BC%E5%B0%BC%E6%A0%BC%E5%B0%94/"},{"name":"彼得·蒂尔","slug":"彼得·蒂尔","link":"/tags/%E5%BD%BC%E5%BE%97%C2%B7%E8%92%82%E5%B0%94/"},{"name":"贾森·弗里德","slug":"贾森·弗里德","link":"/tags/%E8%B4%BE%E6%A3%AE%C2%B7%E5%BC%97%E9%87%8C%E5%BE%B7/"},{"name":"阿利斯泰尔·克罗尔","slug":"阿利斯泰尔·克罗尔","link":"/tags/%E9%98%BF%E5%88%A9%E6%96%AF%E6%B3%B0%E5%B0%94%C2%B7%E5%85%8B%E7%BD%97%E5%B0%94/"},{"name":"吴晓波","slug":"吴晓波","link":"/tags/%E5%90%B4%E6%99%93%E6%B3%A2/"},{"name":"经济","slug":"经济","link":"/tags/%E7%BB%8F%E6%B5%8E/"},{"name":"钱穆","slug":"钱穆","link":"/tags/%E9%92%B1%E7%A9%86/"},{"name":"经典","slug":"经典","link":"/tags/%E7%BB%8F%E5%85%B8/"},{"name":"KG","slug":"KG","link":"/tags/KG/"},{"name":"Recommendation","slug":"Recommendation","link":"/tags/Recommendation/"},{"name":"GPT","slug":"GPT","link":"/tags/GPT/"},{"name":"NLG","slug":"NLG","link":"/tags/NLG/"},{"name":"BERT","slug":"BERT","link":"/tags/BERT/"},{"name":"NLU","slug":"NLU","link":"/tags/NLU/"},{"name":"self-supervised Learning","slug":"self-supervised-Learning","link":"/tags/self-supervised-Learning/"},{"name":"Best Paper","slug":"Best-Paper","link":"/tags/Best-Paper/"},{"name":"best paper","slug":"best-paper","link":"/tags/best-paper/"},{"name":"NMT","slug":"NMT","link":"/tags/NMT/"},{"name":"OpenIE","slug":"OpenIE","link":"/tags/OpenIE/"},{"name":"IE","slug":"IE","link":"/tags/IE/"},{"name":"NER","slug":"NER","link":"/tags/NER/"},{"name":"survey","slug":"survey","link":"/tags/survey/"},{"name":"曹天元","slug":"曹天元","link":"/tags/%E6%9B%B9%E5%A4%A9%E5%85%83/"},{"name":"科普","slug":"科普","link":"/tags/%E7%A7%91%E6%99%AE/"},{"name":"总结","slug":"总结","link":"/tags/%E6%80%BB%E7%BB%93/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"罗伯特・T・清崎","slug":"罗伯特・T・清崎","link":"/tags/%E7%BD%97%E4%BC%AF%E7%89%B9%E3%83%BBT%E3%83%BB%E6%B8%85%E5%B4%8E/"},{"name":"有赞","slug":"有赞","link":"/tags/%E6%9C%89%E8%B5%9E/"},{"name":"kylin","slug":"kylin","link":"/tags/kylin/"},{"name":"彼得·林奇","slug":"彼得·林奇","link":"/tags/%E5%BD%BC%E5%BE%97%C2%B7%E6%9E%97%E5%A5%87/"},{"name":"斌卡","slug":"斌卡","link":"/tags/%E6%96%8C%E5%8D%A1/"},{"name":"健身","slug":"健身","link":"/tags/%E5%81%A5%E8%BA%AB/"}],"categories":[{"name":"认知","slug":"认知","link":"/categories/%E8%AE%A4%E7%9F%A5/"},{"name":"商业","slug":"商业","link":"/categories/%E5%95%86%E4%B8%9A/"},{"name":"历史","slug":"历史","link":"/categories/%E5%8E%86%E5%8F%B2/"},{"name":"nlp","slug":"nlp","link":"/categories/nlp/"},{"name":"物理","slug":"物理","link":"/categories/%E7%89%A9%E7%90%86/"},{"name":"计算机","slug":"计算机","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"},{"name":"金融","slug":"金融","link":"/categories/%E9%87%91%E8%9E%8D/"},{"name":"大数据","slug":"大数据","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"生物","slug":"生物","link":"/categories/%E7%94%9F%E7%89%A9/"}]}